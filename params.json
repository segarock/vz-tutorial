{
  "name": "Руководство по созданию и управлению контейнерами и виртуальными машинами на базе OpenVZ 7",
  "tagline": "",
  "body": "## <a name='toc'></a>Содержание\r\n1. [Введение в виртуализацию](#intro)\r\n  - [Эмуляция оборудования](#emulation)\r\n  - [Полная виртуализация](#full-virt)\r\n  - [Паравиртуализация](#paravirt)\r\n  - [Контейнерная виртуализация (виртуализация уровня ОС)](#cont-virt)\r\n  - [OpenVZ — объединение технологий виртуализации уровня ОС и полной виртуализации](#vz7)\r\n2. [Краткая история проектов Virtuozzo/OpenVZ](#history)\r\n3. [Что нового в OpenVZ 7?](#changes)\r\n4. [Установка и подготовительные действия](#install)\r\n  - [Установка OpenVZ с помощью ISO-образа (bare-metal installation)](#bare-metal)\r\n  - [Подготовительные действия](#prepare)\r\n5. [Управление шаблонами](#templates)\r\n  - [Шаблоны гостевых ОС](#guest-os)\r\n  - [Шаблоны приложений](#app-templates)\r\n  - [Шаблоны контейнеров и виртуальных машин](#templates-сt-vm)\r\n6. [Создание и настройка контейнеров](#ct)\r\n  - [Конфигурационные файлы](#configs)\r\n  - [Создание контейнера](#create-ct)\r\n  - [Настройка контейнера](#setup-ct)\r\n  - [Запуск и вход](#run-enter)\r\n7. [Управление контейнерами](#management-ct)\r\n  - [Управление состоянием контейнера](#status-ct)\r\n  - [Переустановка контейнера](#reinstall-ct)\r\n  - [Клонирование контейнера](#clone-ct)\r\n  - [Запуск команд в контейнере с хост-ноды](#run-commands)\r\n8. [Управление ресурсами контейнеров](#resources-ct)\r\n  - [Дисковые квоты](#quota)\r\n  - [Процессор](#cpu)\r\n  - [Операции ввода/вывода](#io)\r\n  - [Память](#memory)\r\n  - [Мониторинг ресурсов](#monitoring)\r\n9. [Проброс устройств в контейнеры](#forward-dev-ct)\r\n  - [TUN/TAP](#tun-tap)\r\n  - [FUSE](#fuse)\r\n  - [NFS](#nfs)\r\n  - [PPTP](#pptp)\r\n  - [Netfilter/IPTables](#netfilter)\r\n10. [SimFS и ploop](#simfs-ploop)\r\n12. [Управление снапшотами](#snapshots)\r\n13. [Работа с виртуальными машинами](#vm)\r\n  - [Создание и запуск ВМ](#create-vm)\r\n  - [VNC](#vnc)\r\n  - [Дополнения гостевой ОС](#guest-tools)\r\n  - [Приостановка виртуальных машин](#pause-vm)\r\n  - [Шаблоны конфигураций](#templates-vm)\r\n  - [Добавление и удаление устройств в ВМ](#devices-vm)\r\n  - [Горячее подключение CPU и RAM](#hotplug-vm)\r\n  - [Оптимизация виртуальных машин с помощью KSM](#ksm)\r\n14. [Миграция контейнеров и виртуальных машин](#migration)\r\n15. [Расширенная информация о контейнерах и ВМ](#extra-info)\r\n16. [Рекомендации системному администратору](#recommendations)\r\n17. [Ссылки](#links)\r\n18. [TODO](#todo)\r\n19. [Лицензия](#license)\r\n\r\n## [[⬆]](#toc) <a name='intro'></a>Введение в виртуализацию\r\nВиртуализация — предоставление наборов вычислительных ресурсов или их логического объединения, абстрагированных от аппаратной реализации, и обеспечивающих изоляцию вычислительных процессов.\r\n\r\nВиртуализацию можно использовать в:\r\n* консолидации серверов (возможность мигрировать с физических серверов на виртуальные, тем самым увеличивая коэффициент использования аппаратуры, что позволяет существенно сэкономить на аппаратуре, электроэнергии и обслуживании)\r\n* разработке и тестировании приложений (возможность одновременно запускать несколько различных операционных систем (ОС), это удобно при разработке кроссплатформенного программного обеспечения (ПО), таким образом значительно повышается качество, скорость разработки и тестирования приложений)\r\n* бизнесе (использование виртуализации в бизнесе растет с каждым днем и постоянно находятся новые способы применения этой технологии, например, возможность безболезненно сделать снапшот)\r\n* организации виртуальных рабочих станций (так называемых \"тонких клиентов\")\r\n\r\n*Общая схема взаимодействия виртуализации с аппаратным и программным обеспечением*\r\n![Общая схема взаимодействия виртуализации с аппаратным и программным обеспечением](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/virt-scheme.png)\r\n\r\nПонятие виртуализации можно условно разделить на две категории:\r\n* виртуализация платформ (продуктом этого вида виртуализации являются виртуальные машины)\r\n* виртуализация ресурсов (преследует целью комбинирование или упрощение представления аппаратных ресурсов для пользователя и получение неких пользовательских абстракций оборудования, пространств имен, сетей)\r\n\r\nВзаимодействие приложений и операционной системы с аппаратным обеспечением осуществляется через абстрагированный слой виртуализации.\r\n\r\nСуществует несколько подходов организации виртуализации:\r\n* эмуляция оборудования (QEMU, Bochs, Dynamips)\r\n* полная виртуализация (KVM, Hyper-V, VirtualBox, VMware ESXi)\r\n* паравиртуализация (Xen, L4, Trango)\r\n* виртуализация уровня ОС (LXC, OpenVZ, Jails, Solaris Zones)\r\n\r\n### <a name='emulation'></a>Эмуляция оборудования\r\nЭмуляция аппаратных средств является одним из самых сложных методов виртуализации.\r\nВ то же время главной проблемой при эмуляции аппаратных средств является низкая скорость работы, в связи с тем, что каждая команда моделируется на основных аппаратных средствах.\r\n\r\nВ процессе эмуляции оборудования используется механизм динамической трансляции, то есть каждая из инструкций эмулируемой платформы заменяется на заранее подготовленный фрагмент инструкций физического процессора.\r\n\r\n*Эмуляция оборудования моделирует аппаратные средства*\r\n![Схема эмуляции оборудования](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/emulation.png)\r\n\r\nДинамический транслятор позволяет во время исполнения переводить инструкции целевого (гостевого) процессора в инструкции центрального процессора хоста для обеспечения эмуляции.\r\nQEMU обеспечивает динамическую трансляцию преобразованием целевой инструкции в микрооперации, эти микрооперации представляют собой элементы C-кода, которые компилируются в объекты.\r\nЗатем запускается основной транслятор, который отображает целевые инструкции на микрооперации для динамической трансляции.\r\nТакой подход не только эффективен, но и обеспечивает переносимость.\r\n\r\n### <a name='full-virt'></a>Полная виртуализация\r\nПолная виртуализация использует гипервизор, который осуществляет связь между гостевой ОС и аппаратными средствами физического сервера.\r\nВ связи с тем, что вся работа с гостевой операционной системой проходит через гипервизор, скорость работы данного типа виртуализации ниже чем в случае прямого взаимодействия с аппаратурой.\r\nОсновным преимуществом полной виртуализации является то, что в ОС не вносятся никакие изменения, единственное ограничение — операционная система должна поддерживать основные аппаратные средства (AMD SVM или Intel VT).\r\n\r\n*Полная виртуализация использует гипервизор*\r\n![Схема полной виртуализации](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/full-virt.png)\r\n\r\nВ архитектуре KVM виртуальная машина выполняется как обычный Linux-процесс, запланированный стандартным планировщиком Linux.\r\nНа самом деле, виртуальный процессор представляется как обычный Linux-процесс, это позволяет KVM пользоваться всеми возможностями ядра Linux.\r\nЭмуляцией устройств управляет модифицированная версия QEMU, которая обеспечивает эмуляцию BIOS, шины PCI, шины USB, а также стандартный набор устройств, таких как дисковые контроллеры IDE и SCSI, сетевые карты и другие.\r\n\r\n### <a name='paravirt'></a>Паравиртуализация\r\nПаравиртуализация имеет некоторые сходства с полной виртуализацией.\r\nЭтот метод использует гипервизор для разделения доступа к основным аппаратным средствам, но объединяет код, касающийся виртуализации, в непосредственно операционную систему, поэтому недостатком метода является то, что гостевая ОС должна быть изменена для гипервизора.\r\nНо паравиртуализация существенно быстрее полной виртуализации, скорость работы виртуальной машины приближена к скорости реальной, это осуществляется за счет отсутствия эмуляции аппаратуры и учета существования гипервизора при выполнении системных вызовов в коде ядра.\r\nВместо привилегированных операций совершаются гипервызовы обращения ядра гостевой ОС к гипервизору с просьбой о выполнении операции.\r\n\r\n*Паравиртуализация разделяет процесс с гостевой ОС*\r\n![Схема паравиртуализации](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/paravirt.png)\r\n\r\nВ паравиртуальном режиме (PV) оборудование не эмулируется, и гостевая операционная система должна быть специальным образом модифицирована для работы в таком окружении.\r\nНачиная с версии 3.0, ядро Linux поддерживает запуск в паравиртуальном режиме без перекомпиляции со сторонними патчами.\r\nПреимущество режима паравиртуализации состоит в том, что он не требует поддержки аппаратной виртуализации со стороны процессора, а также не тратит вычислительные ресурсы для эмуляции оборудования на шине PCI.\r\n\r\nРежим аппаратной виртуализации (HVM), который появился в Xen, начиная с версии 3.0 гипервизора требует поддержки со стороны оборудования.\r\nВ этом режиме для эмуляции виртуальных устройств используется QEMU, который весьма медлителен несмотря на паравиртуальные драйвера.\r\nОднако со временем поддержка аппаратной виртуализации в оборудовании получила настолько широкое распространение, что используется даже в современных процессорах лэптопов.\r\n\r\n### <a name='cont-virt'></a>Контейнерная виртуализация (виртуализация уровня ОС)\r\nВиртуализация уровня операционной системы отличается от других.\r\nОна использует технику, при которой сервера виртуализируются непосредственно над ОС.\r\nНедостатком метода является то, что поддерживается одна единственная операционная система на физическом сервере, которая изолирует контейнеры друг от друга.\r\nПреимуществом виртуализации уровня ОС является \"родная\" производительность.\r\n\r\n*Виртуализация уровня ОС изолирует серверы*\r\n![Схема виртуализации уровня ОС](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/cont-virt.png)\r\n\r\nВиртуализация уровня ОС — метод виртуализации, при котором ядро операционной системы поддерживает несколько изолированных экземпляров пространства пользователя (контейнеров) вместо одного.\r\nС точки зрения пользователя эти экземпляры полностью идентичны реальному серверу.\r\nДля систем на базе UNIX эта технология может рассматриваться как улучшенная реализация механизма chroot.\r\nЯдро обеспечивает полную изолированность контейнеров, поэтому программы из разных контейнеров не могут воздействовать друг на друга.\r\n\r\n### <a name='vz7'></a>OpenVZ — объединение технологий виртуализации уровня ОС и полной виртуализации\r\nOpenVZ позволяет создавать множество защищенных, изолированных друг от друга контейнеров на одном узле.\r\nПомимо этого разрабатываются возможности по созданию виртуальных машин на базе QEMU/KVM.\r\nУправление контейнерами и виртуальными машинами происходит с помощью специализированных утилит.\r\n\r\n*Архитектура OpenVZ 7*\r\n![Архитектура OpenVZ 7](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/vz7-architect.png)\r\n\r\nКаждый контейнер ведет себя так же, как автономный сервер и имеет собственные файлы, процессы, сеть (IP-адреса, правила маршрутизации).\r\nВ отличие от KVM или Xen, OpenVZ использует одно ядро, которое является общим для всех виртуальных сред.\r\n\r\nКонтейнеры можно разделить на две составляющие:\r\n* ядро (namespaces, cgroups, CRIU, ploop, vcmmd...)\r\n* пользовательские утилиты (prlctl, vzctl, vzpkg, vzlist...)\r\n\r\nNamespaces — пространства имен.\r\nЭто механизм ядра, который позволяет изолировать процессы друг от друга. Изоляция может быть выполнена в шести контекстах (пространствах имен):\r\n* mount — предоставляет процессам собственную иерархию файловой системы и изолирует ее от других таких же иерархий по аналогии с chroot\r\n* PID — изолирует идентификаторы процессов (PID) одного пространства имен от процессов с такими же идентификаторами другого пространства\r\n* network — предоставляет отдельным процессам логически изолированный от других стек сетевых протоколов, сетевой интерфейс, IP-адрес, таблицу маршрутизации, ARP и прочие реквизиты\r\n* IPC — обеспечивает разделяемую память и взаимодействие между процессами\r\n* UTS — изоляция идентификаторов узла, таких как имя хоста (hostname) и домена (domain)\r\n* user — позволяет иметь один и тот же набор пользователей и групп в рамках разных пространств имен, в каждом контейнере может быть свой root и любые другие пользователи и группы\r\n\r\nCGroups (Control Groups) — позволяет ограничить аппаратные ресурсы некоторого набора процессов.\r\nПод аппаратными ресурсами подразумеваются: процессорное время, память, дисковая и сетевая подсистемы.\r\nНабор или группа процессов могут быть определены различными критериями.\r\nНапример, это может быть целая иерархия процессов, получающая все лимиты родительского процесса.\r\nКроме этого возможен подсчет расходуемых группой ресурсов, заморозка (freezing) групп, создание контрольных точек (checkpointing) и их перезагрузка.\r\nДля управления этим полезным механизмом существует специальная библиотека libcgroups, в состав которой входят такие утилиты, как cgcreate, cgexec и некоторые другие.\r\n\r\nCRIU (Checkpoint/Restore In Userspace) — обеспечивает создание контрольной точки для произвольного приложения, а также возобновления работы приложения с этой точки.\r\nОсновной целью CRIU является поддержка миграции контейнеров.\r\nУже поддерживаются такие объекты как процессы, память приложений, открытые файлы, конвейеры, IPC сокеты, TCP/IP и UDP сокеты, таймеры, сигналы, терминалы, файловые дескрипторы.\r\nВ разработке также находится миграция TCP соединений.\r\n\r\nVCMM (Virtuozzo containers memory management) — сервис управления механизмом memory cgroups в пространстве пользователя.\r\nМенеджер памяти 4 поколения управляет memory cgroups, который присутствует в ванильном ядре, поэтому не требует сторонних патчей со стороны OpenVZ.\r\n\r\nПроведенные тестирования показывают, что OpenVZ является одним из наиболее актуальных решений на рынке виртуализации, так как показывает внушительные результаты в различных тестированиях.\r\n\r\n*График времени отклика системы*\r\n![Время отклика системы](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/response-time.png)\r\n\r\nНа графике времени отклика системы можно наблюдать результаты трех тестов — с нагрузкой на систему и виртуальную машину, без нагрузки, нагрузкой только на ВМ.\r\nВо всех тестах OpenVZ показал результаты наименьшего времени отклика, в то время, когда ESXi и Hyper-V показывают оверхед 700-3000%, когда у OpenVZ всего 1-3%.\r\n\r\n*График пропускной способности сети*\r\n![Пропускная способность сети](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/network.png)\r\n\r\nНа втором графике — результаты тестирования пропускной способности сети.\r\nНа графике можно наблюдать, что OpenVZ обеспечивает практическую нативную пропускную способность 10Gb сети (9.7Gbit/s отправка и 9.87Gbit/s прием).\r\n\r\n## [[⬆]](#toc) <a name='history'></a>Краткая история проектов Virtuozzo/OpenVZ\r\nВ 1999 году возникла идея создания Linux-контейнеров, а уже в 2002 году компания SWsoft представила первый релиз коммерческой версии Virtuozzo.\r\nВ том же 2002 году появились первые клиенты в Кремниевой долине.\r\n\r\nВ 2004 году выпуск Virtuozzo для Windows.\r\nВ 2005 году было принято решение о разделении Virtuozzo на два отдельных проекта, свободный OpenVZ (под лицензией GNU GPL) и проприетарный Virtuozzo.\r\n\r\nВ 2006 году OpenVZ стал доступен для Debian Linux, переход к ядру RHEL 4.\r\nВ 2007 году портирован на RHEL 5.\r\n\r\nВ 2011 году появилась идея создания проекта CRIU, OpenVZ портирован на RHEL 6.\r\nВ 2012 году стала доступна CRIU v0.1.\r\n\r\nВ конце 2014 года компания Odin анонсировала открытие кодовой базы Parallels Cloud Server и объединение ее с открытым кодом OpenVZ.\r\n\r\nВ апреле 2015 года открыт репозиторий с ядром RHEL 7 (3.10), в мае были открыты исходные коды пользовательских утилит, а в июне выложены тестовые сборки ISO-образов и RPM-пакеты.\r\n\r\nВ марте 2016 года анонсирован выход Virtuozzo 7.0 Beta, с измененной архитектурой, а также с такими нововведениями, как дополнения гостевых ОС для Linux и Windows.\r\nТакже представлена \"живая\" миграция для контейнеров, реализованная с помощью инструментария CRIU и P.Haul.\r\n\r\n25 июля 2016 года [анонсирован](https://lists.openvz.org/pipermail/announce/2016-July/000664.html) окончательный релиз продукта под именем OpenVZ 7.0.\r\nВ релизе объявлены такие события как отказ от развития SimFS, также объявлено что со следующей версии OpenVZ утилита vzctl будет объявлена устаревшей, рекомендуется вместо нее использовать prlctl или virsh.\r\n\r\n## [[⬆]](#toc) <a name='changes'></a>Что нового в OpenVZ 7?\r\nОсновные изменения по сравнению с OpenVZ 6 (2.6.32):\r\n* OpenVZ 7 базируется на ядре RHEL 7 (3.10)\r\n* благодаря большой интеграции кода в ванильное ядро, количество патчей значительно сократилось, для сравнения число коммитов в 2.6.18 (RHEL 5) равно 264000, для 2.6.32 (RHEL 6) — 202000, для 3.10 (RHEL 7) — 66000 коммитов\r\n* реализация живой миграции с помощью P.Haul и CRIU\r\n* возможность создания виртуальных машин на базе KVM\r\n* отказ от управления контейнерами с помощью vzctl в пользу prlctl и virsh\r\n* использование механизма VCMM для управления памятью\r\n* отказ от развития SimFS в пользу ploop\r\n* унифицированное управление контейнерами и виртуальными машинами с помощью libvirt\r\n* гарантированные лимиты памяти\r\n* горячее подключение CPU/RAM для виртуальных машин, поддержка KSM\r\n* обновленная документация с 2005 года\r\n* интеграция работы с Docker и OpenStack\r\n\r\n## [[⬆]](#toc) <a name='install'></a>Установка и подготовительные действия\r\nНачиная с версии OpenVZ 7.0 доступен только один вариант установки, с помощью ISO-образа дистрибутива.\r\n\r\nУстановка OpenVZ с помощью PXE (Preboot Execution Environment) подробно описана в [документации](https://docs.openvz.org/virtuozzo_7_installation_using_pxe_guide.webhelp/).\r\n\r\n### <a name='bare-metal'></a>Установка OpenVZ с помощью ISO-образа (bare-metal installation)\r\nДистрибутив Virtuozzo Linux (VzLinux) с патчами для ядра RHEL 7, утилитами управления и модифицированным установщиком доступен для скачивания в виде стандартного ISO-образа и в виде netinstall-образа.\r\n\r\nТекущие последние версии ISO-образов доступны по адресу: https://download.openvz.org/virtuozzo/releases/7.0/x86_64/iso/\r\n\r\nПосле записи дистрибутива на носитель, можно приступать к установке OpenVZ.\r\nДля этого необходимо загрузиться с носителя.\r\n\r\n*Экран установки OpenVZ после загрузки с носителя*\r\n![Экран установки OpenVZ](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/install-vz.png)\r\n\r\nУстановка OpenVZ ничем не отличается от установки обычного Linux-дистрибутива.\r\nУстановщик Anaconda предложит установить дату и время, раскладку клавиатуры, языковые параметры.\r\nТакже необходимо будет произвести разметку диска и настроить сеть.\r\nПо умолчанию включен kdump, который позволяет в будущем выяснить причины сбоев в ядре, поэтому рекомендуется его не отключать.\r\n\r\n*Экран установки параметров системы*\r\n![Настройки системы](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/anaconda.png)\r\n\r\n*Пример разметки для 30GB диска*\r\n![Разметка диска](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/partitioning.png)\r\n\r\nНеобходимо для раздела `/` выделить не менее 8GB доступного дискового пространства.\r\nРазмер раздела `swap` равен примерно половине объема оперативной памяти.\r\nВсе остальное дисковое пространство (рекомендуется не менее 30GB) выделяется под раздел `/vz` с файловой системой ext4.\r\n\r\n*Настройки сетевого интерфейса и имени хоста*\r\n![Настройки сети](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/network-install.png)\r\n\r\nТакже необходимо задать пароль пользователя `root` и создать локального пользователя, например `vzuser`.\r\n\r\n*Установка пароля суперпользователя и создание локального пользователя*\r\n![Настройки пользователей](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/user.png)\r\n\r\nПосле установки необходимо перезагрузиться.\r\n\r\nНа этом установка OpenVZ с помощью ISO-образа завершена.\r\n\r\n*Меню загрузчика Grub после установки OpenVZ*\r\n![Grub](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/grub.png)\r\n\r\nПервый вход в систему осуществляется от пользователя `vzuser`, по SSH.\r\n\r\nПример получения прав суперпользователя на сервере:\r\n```\r\nuser@localhost ~ $ ssh vzuser@192.168.0.150\r\nvzuser@192.168.0.150's password: пароль_пользователя_vzuser\r\n[vzuser@vz ~]$ su -\r\nPassword: пароль_пользователя_root\r\n[root@vz ~]#\r\n```\r\n\r\n### <a name='prepare'></a>Подготовительные действия\r\nНа сервере важно всегда обновлять программное обеспечение, так как в новых версиях не только могут добавлять новые возможности, но и исправлять уязвимости.\r\nУказанная ниже команда обновляет все существующие в системе пакеты:\r\n```\r\n[root@vz ~]# yum update\r\n```\r\n\r\nДля сервера важно, чтобы было установлено правильное время.\r\nЧтобы синхронизировать время с интернетом необходимо настроить сервер `ntp`.\r\n\r\nЕсли во время установки ОС, была установлена некорректная временная зона, то можно это сделать позже:\r\n```\r\n[root@vz ~]# timedatectl set-timezone Europe/Moscow\r\n```\r\n\r\nУстановка `ntp` и синхронизация времени с удаленными серверами:\r\n```\r\n[root@vz ~]# yum install ntp\r\n[root@vz ~]# systemctl start ntpd\r\n[root@vz ~]# systemctl enable ntpd\r\n[root@vz ~]# ntpdate -q 0.ru.pool.ntp.org 1.ru.pool.ntp.org\r\n```\r\n\r\n## [[⬆]](#toc) <a name='templates'></a>Управление шаблонами\r\n### <a name='guest-os'></a>Шаблоны гостевых ОС\r\nШаблоны гостевых ОС используются для создания контейнеров.\r\n\r\nПросмотр списка уже имеющихся локальных шаблонов гостевых ОС:\r\n```\r\n[root@vz ~]# vzpkg list -O --with-summary\r\nubuntu-14.04-x86_64                :Ubuntu 14.04 (for AMD64/Intel EM64T) Virtuozzo Template\r\nubuntu-16.04-x86_64                :Ubuntu 16.04 (for AMD64/Intel EM64T) Virtuozzo Template\r\nvzlinux-7-x86_64                   :VzLinux 7 (for AMD64/Intel EM64T) Virtuozzo Template\r\ncentos-7-x86_64                    :Centos 7 (for AMD64/Intel EM64T) Virtuozzo Template\r\ncentos-6-x86_64                    :Centos 6 (for AMD64/Intel EM64T) Virtuozzo Template\r\ndebian-8.0-x86_64                  :Debian 8.0 (for AMD64/Intel EM64T) Virtuozzo Template\r\ndebian-8.0-x86_64-minimal          :Debian 8.0 minimal (for AMD64/Intel EM64T) Virtuozzo Template\r\n```\r\n\r\nУдаленно доступные шаблоны:\r\n```\r\n[root@vz ~]# vzpkg list --available --with-summary\r\ndebian-7.0-x86_64\r\nfedora-22-x86_64\r\nfedora-23-x86_64\r\nsuse-42.1-x86_64\r\nubuntu-14.10-x86_64\r\nubuntu-15.04-x86_64\r\nubuntu-15.10-x86_64\r\nvzlinux-6-x86_64\r\n```\r\n\r\nУстановка шаблона:\r\n```\r\n[root@vz ~]# vzpkg install template ubuntu-16.04-x86_64\r\n```\r\n\r\nАльтернативный вариант установки шаблона:\r\n```\r\n[root@vz ~]# yum install ubuntu-16.04-x86_64-ez\r\n```\r\n\r\nУстановка и обновление кэша шаблона:\r\n```\r\n[root@vz ~]# vzpkg create cache ubuntu-16.04-x86_64\r\n[root@vz ~]# vzpkg update cache ubuntu-16.04-x86_64\r\n```\r\n\r\nЕсли не указывать имя шаблона, то установка или обновление кэша произойдет для всех имеющихся шаблонов.\r\n\r\nПросмотр даты последнего обновления кэша:\r\n```\r\n[root@vz ~]# vzpkg list -O\r\nubuntu-14.04-x86_64\r\nubuntu-16.04-x86_64                2016-05-14 02:10:18\r\nvzlinux-7-x86_64\r\ncentos-7-x86_64\r\ncentos-6-x86_64\r\ndebian-8.0-x86_64\r\ndebian-8.0-x86_64-minimal\r\n```\r\n\r\n### <a name='app-templates'></a>Шаблоны приложений\r\nСуществует возможность установки шаблонов приложений для контейнеров.\r\nОсновное отличие между шаблонами гостевых ОС и шаблонами приложений в том, что шаблоны гостевых ОС используются для создания контейнеров, а шаблоны приложений, обеспечивают установку дополнительного ПО для уже имеющихся контейнеров.\r\n\r\nПросмотр списка шаблонов приложений для `centos-7-x86_64`:\r\n```\r\n[root@vz ~]# vzpkg list centos-7-x86_64\r\ncentos-7-x86_64                     2016-02-09 17:01:05\r\ncentos-7-x86_64      cyrus-imap\r\ncentos-7-x86_64      tomcat\r\ncentos-7-x86_64      php\r\ncentos-7-x86_64      docker\r\ncentos-7-x86_64      mailman\r\ncentos-7-x86_64      spamassassin\r\ncentos-7-x86_64      devel\r\ncentos-7-x86_64      mod_ssl\r\ncentos-7-x86_64      jre\r\ncentos-7-x86_64      vzftpd\r\ncentos-7-x86_64      postgresql\r\ncentos-7-x86_64      mysql\r\ncentos-7-x86_64      jsdk\r\n```\r\n\r\nПример установки шаблона приложений `tomcat` и `jre`:\r\n```\r\n[root@vz ~]# vzpkg list ct5\r\ncentos-7-x86_64                    2016-02-09 17:00:57\r\n[root@vz ~]# vzpkg install ct5 tomcat jre\r\n[root@vz ~]# prlctl exec ct5 systemctl start tomcat\r\n[root@vz ~]# prlctl exec ct5 systemctl is-active tomcat\r\nactive\r\n```\r\n\r\nПосле установки можно проверить список установленных шаблонов для контейнера:\r\n```\r\n[root@vz ~]# vzpkg list ct5\r\ncentos-7-x86_64                    2016-02-09 17:00:57\r\ncentos-7-x86_64      tomcat        2016-02-09 19:56:03\r\ncentos-7-x86_64      jre           2016-02-09 20:03:50\r\n```\r\n\r\nУдаление шаблона приложения из контейнера:\r\n```\r\n[root@vz ~]# vzpkg remove ct5 tomcat\r\nRemoved:\r\n tomcat                 noarch    0:7.0.54-2.el7_1\r\n tomcat-admin-webapps   noarch    0:7.0.54-2.el7_1\r\n tomcat-webapps         noarch    0:7.0.54-2.el7_1\r\n tomcat-lib             noarch    0:7.0.54-2.el7_1\r\n tomcat-el-2.2-api      noarch    0:7.0.54-2.el7_1\r\n\r\n[root@vz ~]# vzpkg list ct5\r\ncentos-7-x86_64                    2016-02-09 17:00:57\r\ncentos-7-x86_64      jre           2016-02-09 20:03:50\r\n```\r\n\r\nСоздадим конфиг, на основе которого будет создаваться контейнер с CentOS 7 с предустановленным Apache Tomcat:\r\n```\r\n[root@vz ~]# cd /etc/vz/conf/\r\n[root@vz conf]# cp ve-basic.conf-sample ve-centos-7-x86_64-tomcat.conf-sample\r\n[root@vz conf]# echo OSTEMPLATE=\".centos-7\" >> ve-centos-7-x86_64-tomcat.conf-sample\r\n[root@vz conf]# echo TEMPLATES=\".tomcat\" >> ve-centos-7-x86_64-tomcat.conf-sample\r\n```\r\n\r\nСоздадим кэш для этого приложения:\r\n```\r\n[root@vz ~]# vzpkg create appcache --config centos-7-x86_64-tomcat\r\n[root@vz ~]# vzpkg list appcache\r\ncentos-7-x86_64                    2016-07-31 03:12:28\r\n     tomcat\r\n```\r\n\r\nНа основе нового конфигурационного файла создадим и запустим контейнер:\r\n```\r\n[root@vz ~]# prlctl create ct5 --config centos-7-x86_64-tomcat --vmtype=ct\r\nCreating the Virtuozzo Container...\r\nThe Container has been successfully created.\r\n[root@vz ~]# prlctl start ct5\r\nStarting the CT...\r\nThe CT has been successfully started.\r\n```\r\n\r\nПроверка предустановки Apache Tomcat в составе шаблона:\r\n```\r\n[root@vz ~]# prlctl exec ct5 rpm -q tomcat\r\ntomcat-7.0.54-2.el7_1.noarch\r\n```\r\n\r\n### <a name='templates-сt-vm'></a>Шаблоны контейнеров и виртуальных машин\r\nПомимо шаблонов гостевых ОС и шаблонов приложений, существует также возможность создания контейнера или виртуальной машины на основе других контейнеров или ВМ.\r\n\r\nПример создания шаблона на основе контейнера `ct2` с установленным веб-сервером nginx:\r\n```\r\n[root@vz ~]# prlctl clone ct2 --name CentOS7+nginx --template\r\nClone the ct2 CT to template CentOS7+nginx...\r\nThe CT has been successfully cloned.\r\n```\r\n\r\nПросмотр списка доступных шаблонов:\r\n```\r\n[root@vz ~]# prlctl list -t\r\nUUID                                    DIST            T  NAME\r\n{c28c09dd-a379-43dd-aae9-3e62f972476a}  centos7         CT CentOS7+nginx\r\n```\r\n\r\nСоздадим новый контейнер на основе шаблона `CentOS7+nginx`:\r\n```\r\n[root@vz ~]# prlctl create ct3 --ostemplate CentOS7+nginx\r\nCreating the VM on the basis of the CentOS7+nginx template...\r\nClone the CentOS7+nginx CT to CT ct3...\r\nThe CT has been successfully cloned.\r\n```\r\n\r\nУдаление шаблона:\r\n```\r\n[root@vz ~]# prlctl delete CentOS7+nginx\r\nRemoving the CT...\r\nThe CT has been successfully removed.\r\n```\r\n\r\n## [[⬆]](#toc) <a name='ct'></a>Создание и настройка контейнеров\r\n### <a name='configs'></a>Конфигурационные файлы\r\nВ старых версиях OpenVZ основным идентификатором контейнера является CTID, который вручную указывался при создании контейнера.\r\nСейчас в этом нет необходимости, на смену CTID пришел UUID, который создается автоматически.\r\n\r\nКаждый контейнер имеет свой конфигурационный файл, который хранится в каталоге `/etc/vz/conf/`.\r\nИменуются конфиги по UUID контейнера.\r\nНапример, для контейнера с `UUID = {3d32522a-80af-4773-b9fa-ea4915dee4b3}`, конфиг будет называться `3d32522a-80af-4773-b9fa-ea4915dee4b3.conf`.\r\n\r\nПри создании контейнера можно использовать типовую конфигурацию.\r\nТиповые файлы конфигураций находятся в том же каталоге `/etc/vz/conf/`:\r\n```\r\n[root@vz ~]# ls /etc/vz/conf/ | grep sample\r\nve-basic.conf-sample\r\nve-confixx.conf-sample\r\nve-vswap.1024MB.conf-sample\r\nve-vswap.2048MB.conf-sample\r\nve-vswap.256MB.conf-sample\r\nve-vswap.512MB.conf-sample\r\nve-vswap.plesk.conf-sample\r\nvps.vzpkgtools.conf-sample\r\n```\r\n\r\nВ этих конфигурационных файлах описаны контрольные параметры ресурсов, выделенное дисковое пространство, оперативная память и т.д.\r\nНапример, при использовании конфига `ve-vswap.512MB.conf-sample`, создается контейнер с дисковым пространством 10GB, оперативной памятью 512MB и swap 512MB:\r\n```\r\n[root@vz ~]# egrep \"DISKSPACE|PHYSPAGES|SWAPPAGES|DISKINODES\" /etc/vz/conf/ve-vswap.512MB.conf-sample\r\nPHYSPAGES=\"131072:131072\"\r\nSWAPPAGES=\"131072\"\r\nDISKSPACE=\"10485760:10485760\"\r\nDISKINODES=\"655360:655360\"\r\n```\r\n\r\nЭто удобно, так как существует возможность создавать свои конфигурационные файлы для различных вариаций контейнеров.\r\nСоздадим свой конфигурационный файл, на базе уже существующего `vswap.512MB`.\r\nИсправим в нем только значения `PHYSPAGES`, `SWAPPAGES`, `DISKSPACE`, `DISKINODES`:\r\n```\r\n[root@vz ~]# cp /etc/vz/conf/ve-vswap.512MB.conf-sample /etc/vz/conf/ve-vswap.1GB.conf-sample\r\n[root@vz ~]# vim /etc/vz/conf/ve-vswap.1GB.conf-sample\r\nPHYSPAGES=\"262144:262144\"\r\nSWAPPAGES=\"262144\"\r\nDISKSPACE=\"20971520:20971520\"\r\nDISKINODES=\"1310720:1310720\"\r\n```\r\nТаким образом, при использовании этого конфигурационного файла, будет создаваться контейнер, которому будет доступно 20GB выделенного дискового пространства, 1GB оперативной памяти и 1GB swap.\r\n\r\nУстановка конфигурационного файла шаблона на примере `vswap.1GB` (контейнер должен быть создан):\r\n```\r\n[root@vz ~]# prlctl set ct1 --applyconfig vswap.1GB\r\nThe CT has been successfully configured.\r\n```\r\n\r\n### <a name='create-ct'></a>Создание контейнера\r\nВ качестве параметра к идентификатору контейнера может использоваться любое имя:\r\n```\r\n[root@vz ~]# prlctl create ct1 --ostemplate debian-8.0-x86_64 --vmtype=ct\r\nCreating the Virtuozzo Container...\r\nThe Container has been successfully created.\r\n```\r\n\r\nТаким образом был создан контейнер с именем `ct1` на базе шаблона `debian-8.0-x86_64`.\r\n\r\nТеперь можно просмотреть список имеющихся в системе контейнеров:\r\n```\r\n[root@vz ~]# prlctl list -a\r\nUUID                                    STATUS       IP_ADDR         T  NAME\r\n{3d32522a-80af-4773-b9fa-ea4915dee4b3}  stopped      -               CT ct1\r\n```\r\n\r\nЕсли же при создании контейнера не указывать желаемый шаблон, то OpenVZ будет использовать шаблон по умолчанию.\r\nКонфигурационный файл, в котором указаны директивы по умолчанию `/etc/vz/vz.conf`.\r\nПо умолчанию, используется шаблон `centos-7` и конфигурационный файл `basic`:\r\n```\r\n[root@vz ~]# egrep \"CONFIGFILE|DEF_OSTEMPLATE\" /etc/vz/vz.conf\r\nCONFIGFILE=\"basic\"\r\nDEF_OSTEMPLATE=\".centos-7\"\r\n```\r\n\r\nЕсли планируется создание большого количества однотипных контейнеров, основываясь на одном и том же конфиге, то значения можно исправить на нужные.\r\n\r\n### <a name='setup-ct'></a>Настройка контейнера\r\nКонтейнер создан, его можно запускать.\r\nНо перед первым запуском необходимо установить его IP-адреса, hostname, указать DNS сервера и задать пароль суперпользователя.\r\n\r\nДобавление IP-адресов:\r\n```\r\n[root@vz ~]# prlctl set ct1 --ipadd 192.168.0.161/24\r\n[root@vz ~]# prlctl set ct1 --ipadd fe80::20c:29ff:fe01:fb08\r\n```\r\n\r\nУстановка DNS серверов и hostname:\r\n```\r\n[root@vz ~]# prlctl set ct1 --nameserver 192.168.0.1,192.168.0.2\r\n[root@vz ~]# prlctl set ct1 --hostname ct1.vz.localhost\r\n```\r\n\r\nУстановка поискового домена:\r\n```\r\n[root@vz ~]# prlctl set ct1 --searchdomain 192.168.0.1\r\n```\r\n\r\nУстановка пароля суперпользователя:\r\n```\r\n[root@vz ~]# prlctl set ct1 --userpasswd root:eVjfsDkTE63s5Nw\r\n```\r\n\r\nСгенерировать пароль можно штатными средствами Linux:\r\n```\r\n[root@vz ~]# cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 15 | head -1\r\nBC4Ce984DBWVcXc\r\n```\r\n\r\nИли воспользоваться утилитой `pwgen`:\r\n```\r\n[root@vz ~]# yum localinstall https://dl.fedoraproject.org/pub/epel/7/x86_64/p/pwgen-2.07-1.el7.x86_64.rpm\r\n[root@vz ~]# pwgen -s 15 1\r\nesxrcH7dyoA46LY\r\n```\r\n\r\nПароль будет установлен в контейнер, в файл `/etc/shadow` и не будет сохранен в конфигурационный файл контейнера.\r\nЕсли же пароль будет утерян или забыт, то можно будет просто задать новый.\r\n\r\nДля запуска контейнера при старте хост-ноды добавляем:\r\n```\r\n[root@vz ~]# prlctl set ct1 --onboot yes\r\n```\r\n\r\nТакже можно добавить краткое описание контейнера:\r\n```\r\n[root@vz ~]# prlctl set ct1 --description \"This is my first container\"\r\n```\r\n\r\n### <a name='run-enter'></a>Запуск и вход\r\nЗапуск контейнера:\r\n```\r\n[root@vz ~]# prlctl start ct1\r\nStarting the CT...\r\nThe CT has been successfully started.\r\n```\r\n\r\nПроверка сетевых интерфейсов внутри гостевой ОС:\r\n```\r\n[root@vz ~]# prlctl exec ct1 ip addr show venet0\r\n2: venet0: <BROADCAST,POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default\r\n    link/void\r\n    inet 127.0.0.1/32 scope host venet0\r\n       valid_lft forever preferred_lft forever\r\n    inet 192.168.0.161/24 brd 192.168.0.255 scope global venet0:0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::2/128 scope global\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::20c:29ff:fe01:fb08/64 scope link\r\n       valid_lft forever preferred_lft forever\r\n```\r\n\r\nПроверка корректности hostname:\r\n```\r\n[root@vz ~]# prlctl exec ct1 hostname\r\nct1.vz.localhost\r\n```\r\n\r\nПроверка доступности контейнера в сети и корректность пароля суперпользователя:\r\n```\r\n[root@vz ~]# ssh root@192.168.0.161\r\nroot@192.168.0.161's password: eVjfsDkTE63s5Nw\r\nroot@ct1:~#\r\n```\r\n\r\nВход в контейнер напрямую с хост-ноды:\r\n```\r\n[root@vz ~]# prlctl enter ct1\r\nentered into CT\r\nroot@ct1:/# exit\r\nlogout\r\n[root@vz ~]#\r\n```\r\n\r\nПереход в консоль контейнера:\r\n```\r\n[root@vz ~]# prlctl console ct2\r\nAttached to CT 9d921e42-1087-45e6-bea0-3d706b2d1862 tty2 (type ESC . to detach)\r\n\r\nCentOS Linux 7 (Core)\r\nKernel 3.10.0-327.3.1.vz7.10.11 on an x86_64\r\n\r\nct2 login: root\r\nPassword: veig7Ei1iedeVa8\r\n[root@ct2 ~]#\r\n```\r\n\r\nДля выхода из консоли необходимо использовать комбинацию клавиш `ESC` + `.`\r\n\r\n## [[⬆]](#toc) <a name='management-ct'></a>Управление контейнерами\r\n### <a name='status-ct'></a>Управление состоянием контейнера\r\nСтатус контейнера:\r\n```\r\n[root@vz ~]# prlctl status ct1\r\nCT ct1 exist running\r\n[root@vz ~]# prlctl status ct2\r\nCT ct2 exist stopped\r\n```\r\n\r\nПо выводу команды можно увидеть, что контейнер `ct1` запущен, а контейнер `ct2` остановлен.\r\n\r\nОстановка контейнера:\r\n```\r\n[root@vz ~]# prlctl stop ct1\r\nStopping the CT...\r\nThe CT has been successfully stopped.\r\n```\r\n\r\nИногда нужно выключить контейнер как можно быстрее, например если он был подвержен взлому или создает критическую нагрузку на хост-ноду.\r\nДля того чтобы срочно выключить контейнер, нужно использовать ключ `--kill`:\r\n```\r\n[root@vz ~]# prlctl stop ct1 --kill\r\nStopping the CT...\r\nThe CT has been forcibly stopped\r\n```\r\n\r\nПерезапуск контейнера:\r\n```\r\n[root@vz ~]# prlctl restart ct1\r\nRestarting the CT...\r\nThe CT has been successfully restarted.\r\n```\r\n\r\nПриостановка контейнера сохраняет текущее состояние контейнера в файл, позволяя восстановить контейнер в то же состояние, в котором он был приостановлен, это может быть полезно, к примеру если перезагружается хост-нода и нужно сохранить состояние процессов в контейнере.\r\n\r\nПараметр `suspend` приостанавливает контейнера, а `resume` — восстанавливает:\r\n```\r\n[root@vz ~]# prlctl suspend ct1\r\nSuspending the CT...\r\nThe CT has been successfully suspended.\r\n[root@vz ~]# prlctl status ct1\r\nCT ct1 exist suspended\r\n[root@vz ~]# prlctl resume ct1\r\nResuming the CT...\r\nThe CT has been successfully resumed.\r\n```\r\n\r\nДля удаления контейнера существует параметр `delete` (перед удалением, контейнер нужно сначала остановить):\r\n```\r\n[root@vz ~]# prlctl stop ct1\r\nStopping the CT...\r\nThe CT has been successfully stopped.\r\n[root@vz ~]# prlctl delete ct1\r\nRemoving the CT...\r\nThe CT has been successfully removed.\r\n```\r\n\r\nКоманда выполняет удаление частной области сервера (`/vz/private/$UUID`) и переименовывает файл конфигурации (`/etc/vz/conf/$UUID.conf`), дописывая к его имени окончание `.destroyed`.\r\n\r\nДля того чтобы смонтировать содержимое контейнера без его запуска существует опция `mount`, для размонтирования — `umount`.\r\nЭто может пригодиться например для того, чтобы поправить конфигурационные файлы контейнера с хост-ноды, если контейнер не стартует:\r\n```\r\n[root@vz ~]# prlctl list ct6\r\nUUID                                    STATUS       IP_ADDR         T  NAME\r\n{8de0101f-c166-42ce-ad53-a7900b223d46}  stopped      192.168.0.166   CT ct6\r\n[root@vz ~]# ls /vz/root/8de0101f-c166-42ce-ad53-a7900b223d46/\r\n[root@vz ~]# prlctl mount ct6\r\nMounting the CT...\r\nThe CT has been successfully mounted.\r\n[root@vz ~]# ls /vz/root/8de0101f-c166-42ce-ad53-a7900b223d46/\r\nbin  boot  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\r\n[root@vz ~]# prlctl status ct6\r\nCT ct6 exist mounted\r\n[root@vz ~]# prlctl umount ct6\r\nUnmounting the CT...\r\nThe CT has been successfully unmounted.\r\n[root@vz ~]# ls /vz/root/8de0101f-c166-42ce-ad53-a7900b223d46/\r\n[root@vz ~]#\r\n```\r\n\r\nПереместить частную область контейнера в другую область можно с помощью параметра `move`:\r\n```\r\n[root@vz ~]# mkdir /home/private\r\n[root@vz ~]# prlctl move ct6 --dst /home/private\r\nMove the ct6 CT to /home/private...\r\nThe CT has been successfully moved.\r\n[root@vz ~]# ls /home/private/8de0101f-c166-42ce-ad53-a7900b223d46/\r\ndump/       fs/         .lck        .owner      root.hdd/   scripts/    templates/  .uptime     ve.conf     .ve.layout  .ve.xml\r\n```\r\n\r\nСброс аптайма контейнера:\r\n```\r\n[root@vz ~]# prlctl list -i ct1 | grep Uptime\r\nUptime: 01:15:57 (since 2016-07-31 01:18:41)\r\n[root@vz ~]# prlctl reset-uptime ct1\r\nPerforming reset uptime operation to the CT...\r\nThe CT uptime has been successfully reset.\r\n[root@vz ~]# prlctl list -i ct1 | grep Uptime\r\nUptime: 00:00:00 (since 2016-08-17 23:18:22)\r\n```\r\n\r\n### <a name='reinstall-ct'></a>Переустановка контейнера\r\nДля переустановки ОС в контейнере, существует команда `vzctl reinstall`.\r\n\r\nПереустановка и старт контейнера (для переустановки нужно сначала остановить контейнер):\r\n```\r\n[root@vz ~]# prlctl stop ct1\r\nStopping the CT...\r\nThe CT has been successfully stopped.\r\n[root@vz ~]# vzctl reinstall ct1 --skipbackup\r\n...\r\nContainer was successfully reinstalled\r\n[root@vz ~]# prlctl start ct1\r\nStarting the CT...\r\nThe CT has been successfully started.\r\n```\r\n\r\nПо умолчанию, `vzctl reinstall` без дополнительных параметров, сохраняет все файлы (частную область) прошлого контейнера  в каталог `/old` нового контейнера.\r\nДля того, чтобы не копировать частную область предыдущего контейнера, необходимо использовать ключ `--skipbackup`.\r\n\r\n### <a name='clone-ct'></a>Клонирование контейнера\r\nOpenVZ позволяет клонировать контейнеры:\r\n```\r\n[root@vz ~]# prlctl clone ct1 --name ct2\r\nClone the ct1 CT to CT ct2...\r\nThe CT has been successfully cloned.\r\n[root@vz ~]# prlctl list -a\r\nUUID                                    STATUS       IP_ADDR         T  NAME\r\n{3d32522a-80af-4773-b9fa-ea4915dee4b3}  running      192.168.0.161   CT ct1\r\n{54bc2ba6-b040-469e-9fda-b0eabda822d4}  stopped      192.168.0.161   CT ct2\r\n```\r\n\r\nПри клонировании контейнера необходимо помнить о смене IP-адреса, иначе при попытке запуска будет наблюдаться ошибка:\r\n```\r\n[root@vz ~]# prlctl start ct2\r\nStarting the CT...\r\nFailed to start the CT: PRL_ERR_VZCTL_OPERATION_FAILED\r\nUnable to add ip 192.168.0.161: Address already in use\r\nFailed to start the Container\r\n```\r\n\r\nСначала нужно удалить старые IP-адреса:\r\n```\r\n[root@vz ~]# prlctl set ct2 --ipdel 192.168.0.161/24\r\n[root@vz ~]# prlctl set ct2 --ipdel fe80::20c:29ff:fe01:fb08\r\n```\r\n\r\nЗатем добавить новые:\r\n```\r\n[root@vz ~]# prlctl set ct2 --ipadd 192.168.0.162/24\r\n[root@vz ~]# prlctl set ct2 --ipadd fe80::20c:29ff:fe01:fb09\r\n```\r\n\r\nСмена hostname:\r\n```\r\n[root@vz ~]# prlctl set ct2 --hostname ct2.vz.localhost\r\nThe CT has been successfully configured.\r\n```\r\n\r\nПосле этого контейнер можно запустить:\r\n```\r\n[root@vz ~]# prlctl start ct2\r\nStarting the CT...\r\nThe CT has been successfully started.\r\n```\r\n\r\n### <a name='run-commands'></a>Запуск команд в контейнере с хост-ноды\r\nПример запуска команды в контейнере:\r\n```\r\n[root@vz ~]# prlctl exec ct1 cat /etc/issue\r\nDebian GNU/Linux 8 \\n \\l\r\n```\r\n\r\nИногда бывает нужно выполнить команду на нескольких контейнерах.\r\nДля этого можно использовать конструкцию:\r\n```\r\n[root@vz ~]# CMD=\"cat /etc/issue\"\r\n[root@vz ~]# for i in `prlctl list -o name -H`; do echo \"CT $i\"; prlctl exec $i $CMD; done\r\nCT ct1\r\nDebian GNU/Linux 8 \\n \\l\r\n\r\nCT ct2\r\nDebian GNU/Linux 8 \\n \\l\r\n```\r\n\r\n## [[⬆]](#toc) <a name='resources-ct'></a>Управление ресурсами контейнеров\r\nДоступные контейнеру ресурсы контролируются с помощью набора параметров управления ресурсами.\r\nВсе эти параметры можно редактировать в файлах шаблонов, в каталоге `/etc/vz/conf/`.\r\nИх можно установить вручную, редактируя соответствующие конфиги или используя утилиты OpenVZ.\r\n\r\nПараметры контроля ресурсов контейнеров условно разделяют на группы:\r\n* дисковые квоты\r\n* процессор\r\n* операции ввода/вывода\r\n* память\r\n* сеть\r\n\r\n### <a name='quota'></a>Дисковые квоты\r\nАдминистратор сервера OpenVZ может устанавливать дисковые квоты, в терминах дискового пространства и количества inodes, число которых примерно равно количеству файлов.\r\nЭто первый уровень дисковой квоты.\r\nВ дополнение к этому, администратор может использовать обычные утилиты внутри окружения, для настроек стандартных дисковых квот UNIX для пользователей и групп.\r\n\r\nДля использования дисковых квот, соответствующая директива должна присутствовать в конфигурационном файле OpenVZ:\r\n```\r\n[root@vz ~]# grep DISK_QUOTA /etc/vz/vz.conf\r\nDISK_QUOTA=yes\r\n```\r\n\r\nОсновные параметры:\r\n* `DISKSPACE` — общий размер дискового пространства (задается в Kb)\r\n* `DISKINODES` — общее число дисковых inodes\r\n* `QUOTATIME` — время (в секундах) на которое контейнер может превысить значение soft предела\r\n\r\nПараметры записываются в конфигурационный файл в виде:\r\n```\r\nCOMMAND=\"softlimit:hardlimit\"\r\n```\r\nгде:\r\n* `COMMAND` — команда (`DISKSPACE` или `DISKINODES`)\r\n* `softlimit` — значение которое превышать нежелательно, после пересечения этого предела наступает grace период, по истечении которого, дисковое пространство или inodes прекратят свое существование\r\n* `hardlimit` — значение которое превысить нельзя\r\n\r\nНапример, запись:\r\n```\r\nDISKSPACE=\"19922944:20971520\"\r\nDISKINODES=\"1300000:1310720\"\r\nQUOTATIME=\"600\"\r\n```\r\nозначает, что задается `softlimit` для дискового пространства равным ~19G и `hardlimit` равный 20G, то же самое с inodes 1300000 и 1310720 соответственно.\r\n\r\nЕсли размер занятого дискового пространства или inodes будет выше `softlimit`, то в течении 600 сек (10 мин), в случае не освобождения дискового пространства или inodes, они прекратят свое существование.\r\n\r\nАналогично, можно установить эти параметры с помощью `vzctl`:\r\n```\r\n[root@vz ~]# vzctl set ct1 --diskspace 5G:6G --save\r\nResize the image /vz/private/3d32522a-80af-4773-b9fa-ea4915dee4b3/root.hdd to 6291456K\r\ndumpe2fs 1.42.9 (28-Dec-2013)\r\n[root@vz ~]# vzctl set ct1 --diskinodes 10000:110000 --save\r\n```\r\n\r\n### <a name='cpu'></a>Процессор\r\nПланировщик процессора в OpenVZ также двухуровневый.\r\nНа первом уровне планировщик решает, какому контейнеру дать квант процессорного времени, базируясь на значении параметра `CPUUNITS` для контейнера.\r\nНа втором уровне стандартный планировщик GNU/Linux решает, какому процессу в выбранном контейнере дать квант времени, базируясь на стандартных приоритетах процесса.\r\n\r\nОсновными параметрами управления CPU являются:\r\n* `CPUUNITS` — гарантируемое минимальное количество времени процессора, которое получит соответствующий контейнер\r\n* `CPUMASK` — привязка контейнера к конкретным процессорам, по умолчанию нагрузка распределяется на все процессоры\r\n* `CPULIMIT` — верхний лимит процессорного времени в процентах\r\n* `CPUS` — количество используемых процессорных ядер контейнером\r\n* `NODEMASK` — привязка ядер NUMA-систем к контейнеру\r\n\r\nПараметр `CPUUNITS` указывает процессорное время доступное для контейнера.\r\nПо умолчанию для каждого контейнера это значение равно 1000.\r\nТо есть, если для контейнера `ct1` установить значение 2000, а для контейнера `ct2` оставить значение 1000, то при равных условиях контейнер `ct1` получит ровно в два раза больше процессорного времени.\r\n```\r\n[root@vz ~]# prlctl set ct1 --cpuunits 2000\r\nset cpuunits 2000\r\n```\r\n\r\nЕсли система многопроцессорная, то установка параметра `CPUMASK` может пригодиться для привязки контейнера к конкретным процессорам.\r\nВ случае восьмипроцессорной системы можно, к примеру, привязать контейнер к процессорам 0-3, 6, 7:\r\n```\r\n[root@vz ~]# prlctl set ct1 --cpumask 0-3,6,7\r\nset cpu mask 0-3,6,7\r\n```\r\n\r\nПараметр `CPULIMIT` указывает общий верхний лимит процессорного времени для всех ядер процессора:\r\n```\r\n[root@vz ~]# prlctl set ct1 --cpulimit 15\r\nset cpulimit 15%\r\n```\r\nДля одноядерного процессора верхний лимит будет равен 100%, для двухядерного 200% и так далее.\r\n\r\nСуществует также возможность задания `CPULIMIT` в абсолютных значениях (MHz):\r\n```\r\n[root@vz ~]# prlctl set ct1 --cpulimit 600m\r\nset cpulimit 600Mhz\r\n```\r\n\r\nВ параметре `CPUS` задается число доступных для контейнера процессорных ядер.\r\nКонтейнер по умолчанию получает в использование все процессорные ядра:\r\n```\r\n[root@vz ~]# CPUINFO=\"grep processor /proc/cpuinfo\"\r\n[root@vz ~]# prlctl exec ct1 $CPUINFO\r\nprocessor\t: 0\r\nprocessor\t: 1\r\nprocessor\t: 2\r\nprocessor\t: 3\r\n```\r\n\r\nУстановим для контейнера лимит в 2 процессорных ядра:\r\n```\r\n[root@vz ~]# prlctl set ct1 --cpus 2\r\nset cpus(4): 2\r\n[root@vz ~]# prlctl exec ct1 $CPUINFO\r\nprocessor\t: 0\r\nprocessor\t: 1\r\n```\r\n\r\nДля систем архитектуры NUMA существует возможность привязки контейнера к процессорам NUMA-нод:\r\n```\r\n[root@vz ~]# prlctl set ct1 --nodemask 0\r\n```\r\n\r\nУтилиты контроля ресурсов процессора, гарантируют любому контейнеру количество времени центрального процессора, которое собственно и получает этот контейнер.\r\nПри этом контейнер может потреблять больше времени, чем определено этой величиной, если нет другого конкурирующего с ним за время CPU сервера.\r\n\r\n### <a name='io'></a>Операции ввода/вывода\r\nВ OpenVZ существует возможность управления дисковыми операциями ввода/вывода.\r\nМожно устанавливать значения таких параметров как:\r\n* `IOPRIO`\r\n* `IOLIMIT`\r\n* `IOPSLIMIT`\r\n\r\nПараметр `IOPRIO` указывает приоритет операция ввода вывода для контейнера.\r\nПо умолчанию для всех контейнеров установлен равный приоритет (значение 4).\r\n\r\nИзменение значения параметра можно регулировать от 0 (максимальный приоритет) до 7:\r\n```\r\n[root@vz ~]# prlctl set ct1 --ioprio 6\r\nset ioprio 6\r\n```\r\n\r\nПараметр `IOLIMIT` позволяет ограничивать пропускную способность операций ввода/вывода.\r\nПо умолчанию параметр имеет значение 0, то есть отсутствие лимитов.\r\n\r\nУстановка значения в MB/s:\r\n```\r\n[root@vz ~]# prlctl set ct1 --iolimit 20\r\nSet up iolimit: 20971520\r\n```\r\n\r\nСуществует возможность указания префиксов метрических значений:\r\n* `G` — гигабайт\r\n* `M` — мегабайт\r\n* `K` — килобайт\r\n* `B` — байт\r\n\r\nМаксимальная пропускная способность дисковых операций ввода/вывода составляет 2GB/s.\r\n\r\nПомимо ограничения пропускной способности операций ввода/вывода, существует возможность ограничения количества операций ввода/вывода в секунду.\r\n\r\nПараметр `IOPSLIMT` позволяет установить численное значение операций ввода/вывода в секунду, например 300:\r\n```\r\n[root@vz ~]# prlctl set ct1 --iopslimit 300\r\nset IOPS limit 300\r\n```\r\n\r\nПо умолчанию значение этого параметра равно 0, что означает отсутствие лимитов.\r\n\r\nПроверка ограничения пропускной способностей операций ввода/вывода на примере `IOLIMIT`.\r\n\r\nЗначение `IOLIMIT` равно 0:\r\n```\r\nroot@ct1:/# dd if=/dev/zero of=test bs=1048576 count=10\r\n10+0 records in\r\n10+0 records out\r\n10485760 bytes (10 MB) copied, 0.210523 s, 49.8 MB/s\r\n```\r\n\r\nЗначение `IOLIMIT` равно 500K:\r\n```\r\nroot@ct1:/# dd if=/dev/zero of=test bs=1048576 count=10\r\n10+0 records in\r\n10+0 records out\r\n10485760 bytes (10 MB) copied, 17.4388 s, 601 kB/s\r\n```\r\n\r\n### <a name='memory'></a>Память\r\nВ OpenVZ используется управление памятью четвертого поколения с помощью VCMM.\r\nВ прошлом же использовалось управление памятью с помощью:\r\n* VSwap (третье поколение)\r\n* SLM (второе поколение)\r\n* User Beancounters (первое поколение)\r\n\r\nС пользовательской стороны управление памятью с помощью VSwap и VCMM ничем не отличаются, однако с точки зрения реализации, VCMM уже находится в ванильном ядре и не требует патчей со стороны разработчиков OpenVZ.\r\n\r\nОграничения физической памяти и swap задаются в конфигурационном файле контейнера параметрами `PHYSPAGES` и `SWAPPAGES`.\r\nЗначения устанавливаются в блоках, например:\r\n```\r\nPHYSPAGES=\"262144:262144\"\r\nSWAPPAGES=\"262144:262144\"\r\n```\r\nравняются значениям в 1024MB (262144 блок / 256 = 1024MB).\r\n\r\nС помощью `prlctl` значения параметров можно указывать в метрической системе:\r\n```\r\n[root@vz ~]# prlctl set ct1 --memsize 1G --swappages 1G\r\nSet the memsize parameter to 1024Mb.\r\nSet swappages 262144\r\n```\r\n\r\nOvercommiting — возможность использования большего числа ресурсов, чем выдано контейнеру.\r\n\r\nЗначение `VM_OVERCOMMIT` указывает число, во сколько раз больше памяти сможет использовать контейнер в случае необходимости.\r\nПо умолчанию значение `VM_OVERCOMMIT` равно 1.5.\r\nТо есть для контейнера установлено, с 1024MB оперативной памяти и 1024MB swap, суммарно доступно 2048MB памяти, в случае необходимости контейнер сможет использовать (2048MB * 1.5 = 3072MB) памяти.\r\n\r\nДля изменения значения достаточно прописать параметр в конфигурационный файл контейнера и перезапустить его:\r\n```\r\nVM_OVERCOMMIT=\"2\"\r\n```\r\n\r\nТакже возможна установка параметра с помощью `vzctl`:\r\n```\r\n[root@vz ~]# vzctl set ct1 --vm_overcommit 2 --save\r\n```\r\n\r\nПри использовании значения 2 для ранее упомянутого контейнера с 2048MB памяти, будет доступно (2048MB * 2 = 4096MB) памяти.\r\nЕстественно, если если эти ресурсы доступны на хост-ноде.\r\n\r\n### <a name='monitoring'></a>Мониторинг ресурсов\r\nС помощью утилиты `vznetstat` можно увидеть входящий и исходящий трафик (в байтах и пакетах) для всех контейнеров:\r\n```\r\n[root@vz ~]# vznetstat\r\nUUID                                 Net.Class  Input(bytes) Input(pkts)   Output(bytes) Output(pkts)\r\n0                                    0                244486        3024         1567749         2491\r\n54bc2ba6-b040-469e-9fda-b0eabda822d4 0                     0           0               0            0\r\n4730cba8-deed-4168-9f9e-34373e618026 0                     0           0               0            0\r\n3d32522a-80af-4773-b9fa-ea4915dee4b3 0               2925512       49396        49398885        49254\r\n```\r\n\r\nДля конкретного контейнера можно воспользоваться ключом `-v`:\r\n```\r\n[root@vz ~]# vznetstat -v 3d32522a-80af-4773-b9fa-ea4915dee4b3\r\nUUID                                 Net.Class     Input(bytes)  Input(pkts)  Output(bytes)  Output(pkts)\r\n3d32522a-80af-4773-b9fa-ea4915dee4b3         0          2925512        49396       49398885         49254\r\n```\r\n\r\nУтилита `vzstat` позволяет узнать информацию по нагрузке на контейнер, занятым ресурсам и состоянии сети:\r\n```\r\n[root@vz ~]# vzstat -p 3d32522a-80af-4773-b9fa-ea4915dee4b3 -t\r\nloadavg     0 0 0\r\nCTNum       3\r\nprocs       289 1 288 0 0 0 0\r\nCPU         16 0 2 3 95\r\nsched latency   372 9\r\nMem         989 360 0\r\nMem latency 1 0\r\n  ZONE0 (DMA):      size 15MB, act 4MB, inact 4MB, free 4MB (0/0/1)\r\n  ZONE1 (DMA32):    size 1007MB, act 243MB, inact 274MB, free 355MB (43/54/64)\r\n  Mem lat (ms):     A0 1, K0 0, U0 1, K1 0, U1 0\r\n  Slab pages:       62MB/62MB (ino 22MB, de 0MB, bh 1MB, pb 0MB)\r\nSwap        952 952 0.000 0.000\r\nNet stats   0.382 5949 5.542 5820\r\nif br0 stats    0.171 2975 2.771 2910\r\nif lo stats     0.000 0 0.000 0\r\nif virbr1-nic stats 0.000 0 0.000 0\r\nif enp0s3 stats 0.211 2975 2.771 2910\r\nif virbr1 stats 0.000 0 0.000 0\r\nDisks stats     0.000 0.000\r\n\r\n    CTID ST   %VM    %KM        PROC     CPU     SOCK FCNT MLAT IP\r\n```\r\n\r\n`vzpid` позволяет узнать к какому контейнеру принадлежит процесс, это может быть полезно при просмотре списка процессов с хост-ноды и поиска \"процесса-грузчика\":\r\n```\r\n[root@vz ~]# top\r\n...\r\n     PID     USER    PR  NI    VIRT    RES   SHR   S  %CPU  %MEM    TIME+   COMMAND\r\n    5625       33    20   0  364432   6232  1284   S  26.2   0.6  0:03.20   apache2\r\n...\r\n[root@vz ~]# vzpid 5625\r\n Pid                                    VEID      Name\r\n5625    3d32522a-80af-4773-b9fa-ea4915dee4b3   apache2\r\n```\r\n\r\nУтилита `vzps` аналогична утилите `ps`, она позволяет вывести список процессов и их состояние для конкретного контейнера:\r\n```\r\n[root@vz ~]# vzps aufx -E 3d32522a-80af-4773-b9fa-ea4915dee4b3\r\n    USER     PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n       0    2432  0.0  0.0      0     0 ?        S    20:10   0:00 [kthreadd/3d3252]\r\n       0    2433  0.0  0.0      0     0 ?        S    20:10   0:00  \\_ [khelper]\r\n       0    2420  0.0  0.3  28168  3136 ?        Ss   20:10   0:00 init -z\r\n     101    3088  0.0  0.1  26168  1448 ?        Ss   20:10   0:00  \\_ /lib/systemd/systemd-networkd\r\n       0    3117  0.0  0.1  28856  1620 ?        Ss   20:10   0:00  \\_ /lib/systemd/systemd-journald\r\n       0    3135  0.0  0.1  38916  1624 ?        Ss   20:10   0:00  \\_ /lib/systemd/systemd-udevd\r\n       0    3376  0.0  0.3  55156  3128 ?        Ss   20:10   0:00  \\_ /usr/sbin/sshd -D\r\n     102    3380  0.0  0.1  25732  1092 ?        Ss   20:10   0:00  \\_ /lib/systemd/systemd-resolved\r\n       0    3382  0.0  0.1  25884  1120 ?        Ss   20:10   0:00  \\_ /usr/sbin/cron -f\r\n       0    3388  0.0  0.1 182848  1884 ?        Ssl  20:10   0:00  \\_ /usr/sbin/rsyslogd -n\r\n       0    3433  0.0  0.0  12648   840 ?        Ss+  20:10   0:00  \\_ /sbin/agetty --noclear tty2 linux\r\n       0    3434  0.0  0.0  12648   840 ?        Ss+  20:10   0:00  \\_ /sbin/agetty --noclear --keep-baud console 115200 38400 9600 linux\r\n       0    3508  0.0  0.0  20200   956 ?        Ss   20:10   0:00  \\_ /usr/sbin/xinetd -pidfile /run/xinetd.pid -stayalive -inetd_compat -inetd_ipv6\r\n       0    3617  0.0  0.1  65452  1164 ?        Ss   20:10   0:00  \\_ /usr/sbin/saslauthd -a pam -c -m /var/run/saslauthd -n 2\r\n       0    3625  0.0  0.0  65452   836 ?        S    20:10   0:00  |   \\_ /usr/sbin/saslauthd -a pam -c -m /var/run/saslauthd -n 2\r\n       0    3755  0.0  0.2  73496  2724 ?        Ss   20:10   0:00  \\_ /usr/sbin/apache2 -k start\r\n      33    5747  0.2  0.5 363364  5300 ?        Sl   20:46   0:00  |   \\_ /usr/sbin/apache2 -k start\r\n       0    4074  0.0  0.2  36144  2388 ?        Ss   20:10   0:00  \\_ /usr/lib/postfix/master\r\n     105    4081  0.0  0.2  38208  2316 ?        S    20:10   0:00      \\_ pickup -l -t unix -u -c\r\n     105    4082  0.0  0.2  38256  2336 ?        S    20:10   0:00      \\_ qmgr -l -t unix -u\r\n```\r\n\r\nУтилита `vztop` теперь заменена алиасом на `htop` с отображением UUID контейнера, которому принадлежит процесс:\r\n```\r\n[root@vz ~]# which vztop\r\nalias vztop='htop -s CTID'\r\n\t/usr/bin/htop\r\n```\r\n\r\n*Утилита vztop*\r\n![vztop](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/vztop.png)\r\n\r\n## [[⬆]](#toc) <a name='forward-dev-ct'></a>Проброс устройств в контейнеры\r\n### <a name='tun-tap'></a>TUN/TAP\r\nТехнология VPN позволяет устанавливать безопасное сетевое соединение между компьютерами.\r\nДля того чтобы VPN работала в контейнере, необходимо разрешить использование TUN/TAP устройств для контейнера.\r\n\r\n*Схема работы Virtual Private Network*\r\n![VPN](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/vpn.png)\r\n\r\nПо умолчанию модуль TUN уже загружен в ядро, проверить это можно командой `lsmod`:\r\n```\r\n[root@vz ~]# lsmod | grep ^tun\r\ntun                    27183  1\r\n```\r\n\r\nЕсли все-таки модуль отключен, то включить его можно командой `modprobe`:\r\n```\r\n[root@vz ~]# modprobe tun\r\n```\r\n\r\nПроброс модуля TUN в контейнер:\r\n```\r\n[root@vz ~]# vzctl set ct3 --devnodes net/tun:rw --save\r\nSetting devices\r\nCreate /etc/tmpfiles.d/device-tun.conf\r\n[root@vz ~]# prlctl exec ct3 ls -l /dev/net/tun\r\ncrw------- 1 root root 10, 200 Feb 10 13:12 /dev/net/tun\r\n```\r\n\r\nНа этом настройка TUN окончена.\r\nДалее необходимо установить ПО для работы с VPN.\r\nНапример одну из программ:\r\n* [OpenVPN](https://openvpn.net)\r\n* [tinc](https://tinc-vpn.org)\r\n\r\n### <a name='fuse'></a>FUSE\r\nFUSE (Filesystem in Userspace) — модуль ядра Linux, позволяющий создавать виртуальные файловые системы.\r\nFUSE может пригодиться, например при монтировании Яндекс.Диска или других виртуальных файловых систем.\r\n\r\nДля того, чтобы для контейнеров был доступен FUSE, его необходимо включить на хост-ноде:\r\n```\r\n[root@vz ~]# modprobe fuse\r\n[root@vz ~]# lsmod | grep fuse\r\nfuse                  106371  0\r\n```\r\n\r\nТакже необходимо добавить модуль в автозагрузку, чтобы он подгружался автоматически при рестарте хост-ноды:\r\n```\r\n[root@vz ~]# echo fuse >> /etc/modules-load.d/vz.conf\r\n```\r\n\r\nВключение FUSE для контейнера:\r\n```\r\n[root@vz ~]# vzctl set ct3 --devnodes fuse:rw --save\r\nSetting devices\r\nCreate /etc/tmpfiles.d/device-fuse.conf\r\n[root@vz ~]# prlctl exec ct3 ls -l /dev/fuse\r\ncrw------- 1 root root 10, 229 Feb 10 13:42 /dev/fuse\r\n```\r\n\r\nПример подключения Яндекс.Диска в контейнере:\r\n```\r\n[root@vz ~]# prlctl exec ct3 yum install fuse davfs2\r\n[root@vz ~]# prlctl exec ct3 mount -t davfs https://webdav.yandex.ru /mnt/\r\nPlease enter the username to authenticate with server\r\nhttps://webdav.yandex.ru or hit enter for none.\r\n  Username: user\r\nPlease enter the password to authenticate user username with server\r\nhttps://webdav.yandex.ru or hit enter for none.\r\n  Password:  pass\r\n```\r\n\r\n### <a name='nfs'></a>NFS\r\nNFS (Network File System) – это сетевая файловая система, позволяющая пользователям обращаться к файлам и каталогам, расположенным на удаленных компьютерах, как если бы эти файлы и каталоги были локальными.\r\n\r\nПо умолчанию модуль ядра NFS уже включен в OpenVZ, поэтому нужно всего лишь пробросить устройство в контейнер.\r\nПеред пробросом модуля контейнер нужно остановить:\r\n```\r\n[root@vz ~]# prlctl stop ct1\r\n[root@vz ~]# prlctl set ct1 --features nfsd:on\r\nset features: nfsd:on\r\n```\r\n\r\nА после – включить и проверить работоспособность сервиса `nfs`:\r\n```\r\n[root@vz ~]# prlctl start ct1\r\n[root@vz ~]# prlctl exec ct1 systemctl start nfs\r\n[root@vz ~]# prlctl exec ct1 systemctl is-active nfs\r\nactive\r\n```\r\n\r\n### <a name='pptp'></a>PPTP\r\nPPTP (Point-to-Point Tunneling Protocol) — туннельный протокол типа точка-точка, позволяющий компьютеру устанавливать защищенное соединение с сервером за счет создания специального туннеля в стандартной, незащищенной сети.\r\nPPTP может также использоваться для организации туннеля между двумя локальными сетями.\r\n\r\nДля работы PPTP в контейнере OpenVZ нужно включить соответствующие модули ядра на хост-ноде.\r\n\r\nДобавление модулей ядра `ppp_async`, `ppp_deflate`, `ppp_mppe`:\r\n```\r\n[root@vz ~]# modprobe ppp_async\r\n[root@vz ~]# modprobe ppp_deflate\r\n[root@vz ~]# modprobe ppp_mppe\r\n[root@vz ~]# lsmod | grep ppp\r\nppp_mppe               13002  0\r\nppp_deflate            12950  0\r\nzlib_deflate           26914  1 ppp_deflate\r\nppp_async              17413  0\r\nppp_generic            33029  3 ppp_mppe,ppp_async,ppp_deflate\r\nslhc                   13450  1 ppp_generic\r\ncrc_ccitt              12707  1 ppp_async\r\n```\r\n\r\nДобавление модулей в автозагрузку:\r\n```\r\n[root@vz ~]# echo ppp_async >> /etc/modules-load.d/vz.conf\r\n[root@vz ~]# echo ppp_deflate >> /etc/modules-load.d/vz.conf\r\n[root@vz ~]# echo ppp_mppe >> /etc/modules-load.d/vz.conf\r\n```\r\n\r\nОстановка контейнера, проброс устройства и запуск контейнера:\r\n```\r\n[root@vz ~]# prlctl stop ct1\r\n[root@vz ~]# vzctl set ct1 --devnodes ppp:rw --save\r\n[root@vz ~]# prlctl start ct1\r\n```\r\n\r\nПроверка работы сервиса:\r\n```\r\n[root@vz ~]# prlctl enter ct1\r\n[root@ct1 ~]# ls /dev/ppp\r\n/dev/ppp\r\n[root@ct1 ~]# /usr/sbin/pppd\r\n~�}#�!}!}!} }4}\"}&} } } } }%}&}0V��}'}\"}(}\"ty\r\n```\r\n\r\n### <a name='netfilter'></a>Netfilter/IPTables\r\nNetfilter — это межсетевой экран в ядре Linux.\r\n\r\nВ OpenVZ существует возможность включать или отключать Netfilter для контейнера.\r\n\r\nДоступно четыре режима работы Netfilter в контейнере:\r\n* disabled — отключены все модули\r\n* stateless (по умолчанию) — все модули включены, за исключением NAT и conntracks\r\n* stateful — все модули включены, за исключением NAT\r\n* full — все модули включены\r\n\r\nДля изменения параметров Netfilter, контейнер должен быть отключен.\r\n\r\nПример отключения всех модулей Netfilter для контейнера:\r\n```\r\n[root@vz ~]# prlctl set ct1 --netfilter disabled\r\nSet netfilter: disabled\r\n[root@vz ~]# prlctl exec ct1 iptables -L INPUT\r\niptables v1.4.21: can't initialize iptables table `filter': Table does not exist (do you need to insmod?)\r\nPerhaps iptables or your kernel needs to be upgraded.\r\n```\r\n\r\nВключение всех модулей:\r\n```\r\n[root@vz ~]# prlctl set ct1 --netfilter full\r\nSet netfilter: full\r\n[root@vz ~]# prlctl exec ct1 iptables -L INPUT\r\nChain INPUT (policy ACCEPT)\r\ntarget     prot opt source               destination\r\nACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED\r\nACCEPT     icmp --  anywhere             anywhere\r\nACCEPT     all  --  anywhere             anywhere\r\nACCEPT     tcp  --  anywhere             anywhere             state NEW tcp dpt:ssh\r\nREJECT     all  --  anywhere             anywhere             reject-with icmp-host-prohibited\r\n```\r\n\r\n## [[⬆]](#toc) <a name='simfs-ploop'></a>SimFS и ploop\r\nДля работы OpenVZ с файлами контейнера, существует два метода:\r\n* SimFS (каталоги и файлы в файловой системе хост-ноды)\r\n* ploop (отдельный файл для каждого контейнера)\r\n\r\nПо умолчанию в OpenVZ используется ploop.\r\nSimFS уже давно не используется, и с версии OpenVZ 7 больше не будет поддерживаться.\r\nОсновные преимущества ploop:\r\n* поддержка корректной и надежной изоляции пользователей друг от друга\r\n* журнал файловой системы больше не является узким местом\r\n* живая миграция\r\n* поддержка различных типов хранения данных\r\n* быстрое изменение размера контейнера без его отключения\r\n\r\nploop может работать только с файловой системой ext4.\r\n\r\nДля тех, кому требуется использование устаревшего SimFS существует возможность его включения:\r\n```\r\n[root@vz ~]# vim /etc/vz/vz.conf\r\n#VEFSTYPE=\"ext4\"\r\nVEFSTYPE=\"simfs\"\r\n[root@vz ~]# prlctl create ct3 --vmtype=ct\r\n[root@vz ~]# ls /vz/private/731a1572-d609-498f-8c8b-8739e336a210/fs/\r\n.autorelabel  boot/  etc/   lib/     media/  opt/   root/  sbin/  sys/  usr/  .vzfifo\r\nbin/          dev/   home/  lib64/   mnt/    proc/  run/   srv/   tmp/  var/\r\n```\r\n\r\nДля отключения SimFS нужно в файле `/etc/vz/vz.conf` установить переменную `VEFSTYPE=\"ext4\"`.\r\n\r\n## [[⬆]](#toc) <a name='snapshots'></a>Управление снапшотами\r\nПеред созданием снапшота рекомендуется закончить установку ПО, загрузку файлов и запись на внешние устройства.\r\nТакже рекомендуется отменить транзакции во внешние БД.\r\n\r\nСоздание снапшота контейнера:\r\n```\r\n[root@vz ~]# prlctl snapshot ct2 -n FreshCentOS7 -d \"Fresh CentOS 7 container\"\r\nCreating the snapshot...\r\nThe snapshot with id {aa9649d9-9ed1-408a-9463-36ce0cea6ba7} has been successfully created.\r\n```\r\nгде параметр `-n` указываем имя снапшота, `-d` – его описание, описание снапшота всегда указывается в кавычках.\r\n\r\nСнапшоты сохраняются в каталог `/vz/private/$UUID/dump/`:\r\n```\r\n[root@vz ~]# ls /vz/private/49465252-b780-45f1-9784-538166be2367/dump/\r\n{aa9649d9-9ed1-408a-9463-36ce0cea6ba7}  {aa9649d9-9ed1-408a-9463-36ce0cea6ba7}.ve.conf\r\n```\r\n\r\nПодробная информация о снапшоте:\r\n```\r\n[root@vz ~]# prlctl snapshot-list ct2 -i {aa9649d9-9ed1-408a-9463-36ce0cea6ba7}\r\nID: {aa9649d9-9ed1-408a-9463-36ce0cea6ba7}\r\nName: FreshCentOS7\r\nDate: 2016-07-31 01:44:43\r\nState: poweroff\r\nDescription: Fresh CentOS 7 container\r\n```\r\n\r\nСписок доступных снапшотов для контейнера:\r\n```\r\n[root@vz ~]# prlctl snapshot-list ct2\r\nPARENT_SNAPSHOT_ID                      SNAPSHOT_ID\r\n                                        {aa9649d9-9ed1-408a-9463-36ce0cea6ba7}\r\n{aa9649d9-9ed1-408a-9463-36ce0cea6ba7} *{e76f0a0d-aa8e-491f-a720-daa65dfb911a}\r\n```\r\n\r\nСимвол `*` указывает текущую ветку снапшота:\r\n```\r\n[root@vz ~]# prlctl snapshot-list ct2 -t\r\n_{aa9649d9-9ed1-408a-9463-36ce0cea6ba7}*{e76f0a0d-aa8e-491f-a720-daa65dfb911a}\r\n```\r\n\r\nДля проверки работы снапшота создадим файл, а потом восстановимся из снапшота, в котором этого файла нет:\r\n```\r\n[root@vz ~]# prlctl exec ct2 touch /root/file.txt\r\n[root@vz ~]# prlctl exec ct2 ls /root/file.txt\r\n/root/file.txt\r\n[root@vz ~]# prlctl snapshot-switch ct2 --id {aa9649d9-9ed1-408a-9463-36ce0cea6ba7}\r\nSwitch to the snapshot...\r\nThe CT has been successfully switched.\r\n[root@vz ~]# prlctl exec ct2 ls /root/file.txt\r\nls: cannot access /root/file.txt: No such file or directory\r\n```\r\n\r\nУдаление снапшота:\r\n```\r\n[root@vz ~]# prlctl snapshot-delete ct2 --id {aa9649d9-9ed1-408a-9463-36ce0cea6ba7}\r\nDelete the snapshot...\r\nThe snapshot has been successfully deleted.\r\n```\r\n\r\nПри удалении родительского снапшота, снапшоты-потомки не удаляются.\r\n\r\n## [[⬆]](#toc) <a name='vm'></a>Работа с виртуальными машинами\r\nПомимо создания контейнеров, OpenVZ 7 поддерживает создание и управление виртуальными машинами на базе QEMU/KVM.\r\nУтилита `prlctl` имеет возможность создавать и управлять виртуальными машинами, помимо этого также доступно управление с помощью `libvirt`.\r\n\r\n### <a name='create-vm'></a>Создание и запуск ВМ\r\nСоздание виртуальной машины практически ничем не отличается от создания контейнера:\r\n```\r\n[root@vz ~]# prlctl create vm1 --distribution rhel7 --vmtype vm\r\nCreating the virtual machine...\r\nGenerate the VM configuration for rhel7.\r\nThe VM has been successfully created.\r\n```\r\n\r\nКлюч `--distribution` (`-d`) указывает на семейство ОС или дистрибутив для оптимизации виртуального окружения.\r\nСписок всех официально поддерживаемых ОС:\r\n```\r\n[root@vz ~]# prlctl create vm1 -d list\r\nThe following values are allowed:\r\nwin-2000        \twin-xp          \twin-2003        \twin-vista\r\nwin-2008        \twin-7           \twin-8           \twin-2012\r\nwin-8.1         \twin             \trhel            \trhel7\r\nsuse            \tdebian          \tfedora-core     \tfc\r\nxandros         \tubuntu          \tmandriva        \tcentos\r\ncentos7         \tvzlinux7        \tpsbm            \tredhat\r\nopensuse        \tlinux-2.4       \tlinux-2.6       \tlinux\r\nmageia          \tmint            \tfreebsd-4       \tfreebsd-5\r\nfreebsd-6       \tfreebsd-7       \tfreebsd-8       \tfreebsd\r\nchrome-1        \tchrome\r\n```\r\n\r\nДля каждой виртуальной машины в каталоге `/vz/vmprivate/` создается собственная директория с именем, соответствующим ее UUID:\r\n```\r\n[root@vz ~]# ls /vz/vmprivate/1bdb465a-31e6-46c5-ad7b-947f6ff00208/\r\nconfig.pvs  config.pvs.backup  harddisk.hdd\r\n```\r\n\r\nДля каждой ВМ имеется как минимум два файла:\r\n* файл конфигурации `config.pvs`\r\n* виртуальный жесткий диск `harddisk.hdd`\r\n\r\nВ файле конфигурации описываются параметры виртуальной машины в XML-формате.\r\n\r\nВ свою очередь виртуальный жесткий диск может быть двух типов:\r\n* plain — диск с фиксированным размером\r\n* expanded — диск с изменяемым размером\r\n\r\nПо умолчанию при создании виртуальной машины, создается expanded-диск размером 65G.\r\n\r\nПросмотр только что созданной ВМ:\r\n```\r\n[root@vz ~]# prlctl list -a\r\nUUID                                    STATUS       IP_ADDR         T  NAME\r\n{6fe60288-fe50-49fe-a68d-7a8330837358}  stopped      192.168.0.161   CT ct1\r\n{2cdb07fd-a68a-4279-81c1-3d269460c2f7}  stopped      192.168.0.162   CT ct2\r\n{485372f0-2ae3-4bfe-aa55-e556c37fea9f}  stopped      -               VM vm1\r\n```\r\n\r\nПо аналогии с контейнерами установим необходимые параметры для виртуальной машины:\r\n```\r\n[root@vz ~]# prlctl set vm1 --description \"Backend for app\"\r\n[root@vz ~]# prlctl set vm1 --device-set net0 --ipadd 192.168.0.180/24\r\n[root@vz ~]# prlctl set vm1 --device-set net0 --ipadd 192.168.0.180/24\r\n[root@vz ~]# prlctl set vm1 --device-set net0 --ipadd FE80:0:0:0:20C:29FF:FE01:FB07\r\n[root@vz ~]# prlctl set vm1 --nameserver 192.168.0.1,192.168.0.2\r\n[root@vz ~]# prlctl set vm1 --memsize 1024\r\n[root@vz ~]# prlctl set vm1 --cpus 2\r\n[root@vz ~]# prlctl set vm1 --cpuunits 1000\r\n[root@vz ~]# prlctl set vm1 --cpulimit 1024m\r\n[root@vz ~]# prlctl set vm1 --cpumask 0-1\r\n[root@vz ~]# prlctl set vm1 --ioprio 6\r\n[root@vz ~]# prlctl set vm1 --iolimit 0\r\n[root@vz ~]# prlctl set vm1 --iopslimit 0\r\n```\r\n\r\nКлюч `--videosize` указывает размер выделяемой видеопамяти в MB для виртуальной машины:\r\n```\r\n[root@vz ~]# prlctl set vm1 --videosize 64\r\n```\r\n\r\nКлюч `--autostart` аналогичен `--onboot` для контейнеров, который указывает возможность автостарта контейнера при старте хост-ноды:\r\n```\r\n[root@vz ~]# prlctl set vm1 --autostart on\r\n```\r\n\r\nПараметры виртуальной машины установлены, ее можно запускать, однако для установки гостевой ОС необходим соответствующий образ.\r\nДля хранения образов ОС можно создать отдельный каталог и централизованно хранить в нем все образы:\r\n```\r\n[root@vz ~]# mkdir /vz/vmprivate/images/\r\n[root@vz ~]# ls /vz/vmprivate/images/ -1\r\nCentOS-7-x86_64-Minimal-1503-01.iso\r\n9200.16384.WIN8_RTM.120725-1247_X64FRE_SERVER_EVAL_RU-RU-HRM_SSS_X64FREE_RU-RU_DV5.ISO\r\n```\r\n\r\nОзнакомительные образы Windows Server можно найти по адресу: https://www.microsoft.com/ru-ru/evalcenter/evaluate-windows-server-2012\r\n\r\nУстановка ВМ с образа `CentOS-7-x86_64-Minimal-1503-01.iso`:\r\n```\r\n[root@vz ~]# prlctl set vm1 --device-set cdrom1 --image \"/vz/vmprivate/images/CentOS-7-x86_64-Minimal-1503-01.iso\" --iface scsi --position 1\r\n```\r\n\r\nИзменение размера диска до 8G:\r\n```\r\n[root@vz ~]# prl_disk_tool resize --hdd /vz/vmprivate/vm1.pvm/harddisk.hdd --size 8G\r\n```\r\n\r\nПросмотр конфигурации виртуальной машины перед запуском:\r\n```\r\n[root@vz ~]# prlctl list vm1 -i | grep Hardware -A9\r\nHardware:\r\n  cpu cpus=2 VT-x accl=high mode=32 cpuunits=1000 cpulimit=1024Mhz ioprio=6 iolimit='0' mask=0-1\r\n  memory 1024Mb\r\n  video 64Mb 3d acceleration=highest vertical sync=yes\r\n  memory_guarantee auto\r\n  hdd0 (+) scsi:0 image='/vz/vmprivate/vm1.pvm/harddisk.hdd' type='expanded' 8192Mb subtype=virtio-scsi\r\n  cdrom0 (+) ide:0 image='/vz/vmprivate/vm1.pvm/cloud-config.iso'\r\n  cdrom1 (+) scsi:1 image='/vz/vmprivate/images/CentOS-7-x86_64-Minimal-1503-01.iso' subtype=virtio-scsi\r\n  usb (+)\r\n  net0 (+) dev='vme4292dc5f' network='Bridged' mac=001C4292DC5F card=virtio ips='192.168.0.180/255.255.255.0 FE80:0:0:0:20C:29FF:FE01:FB07/64 '\r\n```\r\n\r\n### <a name='vnc'></a>VNC\r\nПодключение VNC для ВМ:\r\n```\r\n[root@vz ~]# prlctl set vm1 --vnc-mode manual --vnc-port 5901 --vnc-passwd Oiwaiqud\r\nConfigure VNC: Remote display: mode=manual port=5901\r\n```\r\n\r\nТакже можно предоставить беспарольный доступ к VNC с помощью ключа `--vnc-nopasswd` вместо `--vnc-passwd`.\r\n\r\nДля каждой виртуальной машины должен быть установлен уникальный порт для VNC.\r\nПо аналогии с виртуальными машинами, VNC доступен и для контейнеров.\r\n\r\nЗапуск виртуальной машины:\r\n```\r\n[root@vz ~]# prlctl start vm1\r\nStarting the VM...\r\nThe VM has been successfully started.\r\n```\r\n\r\nТеперь к ВМ можно подключиться по VNC:\r\n```\r\nuser@localhost ~ $ sudo apt-get install xvnc4viewer\r\nuser@localhost ~ $ xvnc4viewer 192.168.0.150:5901\r\nPassword: Oiwaiqud\r\n```\r\n\r\n*Подключенная VNC-сессия к виртуальной машине*\r\n![VNC](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/vnc.png)\r\n\r\nДалее следует обычная установка ОС в виртуальную машину.\r\nПо окончании установки необходимо перезагрузиться.\r\n\r\n*Установленная гостевая ОС*\r\n![Установленная гостевая ОС](https://raw.githubusercontent.com/Amet13/vz-tutorial/master/images/installed-os.png)\r\n\r\nПосле установки ОС, можно соединиться к виртуальной машине по SSH:\r\n```\r\nuser@localhost ~ $ ssh root@192.168.0.180\r\nroot@192.168.0.180's password: eihaixahghath7A\r\n[root@vm1 ~]#\r\n```\r\n\r\n### <a name='guest-tools'></a>Дополнения гостевой ОС\r\nOpenVZ поддерживает OpenVZ Guest Tools (дополнения гостевой ОС), которые позволяют выполнять некоторые операции в ВМ такие как:\r\n* запуск команд в ВМ с помощью `prlctl exec`\r\n* установка паролей для пользователей с помощью `prlctl set --userpasswd`\r\n* управление сетевыми настройками в ВМ\r\n\r\nУстановка дополнений для `vm1` с хост-ноды:\r\n```\r\n[root@vz ~]# prlctl installtools vm1\r\nInstalling...\r\nThe Parallels tools have been successfully installed.\r\n```\r\n\r\nДалее необходимо войти в ВМ, например по SSH и запустить скрипт установки дополнений:\r\n```\r\n[root@vz ~]# ssh root@192.168.0.180\r\nroot@192.168.0.180's password: eihaixahghath7A\r\n[root@vm1 ~]# mount /dev/cdrom /mnt/\r\nmount: /dev/sr0 is write-protected, mounting read-only\r\n[root@vm1 ~]# bash /mnt/install\r\nPreparing...                          ################################# [100%]\r\nUpdating / installing...\r\n   1:qemu-guest-agent-vz-2.5.0-19.el7 ################################# [100%]\r\nPreparing...                          ################################# [100%]\r\nUpdating / installing...\r\n   1:prl_nettool-7.0.1-3.vz7          ################################# [100%]\r\nPreparing...                          ################################# [100%]\r\nUpdating / installing...\r\n   1:vz-guest-udev-7.0.0-2            ################################# [100%]\r\nDone!\r\n```\r\n\r\nПроверка корректности установки дополнений с хост-ноды:\r\n```\r\n[root@vz ~]# prlctl exec vm1 uname -a\r\nLinux vm1.tld 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n[root@vz ~]# prlctl set vm1 --userpasswd testuser:iel9cophoo2Aisa\r\nAuthentication tokens updated successfully.\r\n[root@vz ~]# prlctl exec vm1 id testuser\r\nuid=1000(testuser) gid=1000(testuser) groups=1000(testuser)\r\n```\r\n\r\nВ гостевой Windows установка дополнений сводится к трем пунктам:\r\n* установка драйвера, который находится в `<CD_root>/vioserial/<Win_version>/amd64/vioser.inf`\r\n* запуск `prl_nettool_<Win_arch>.msi` и `qemu-ga-<Win_arch>.msi`\r\n* проверка работоспособности сервиса `qemu-ga.exe`\r\n\r\nАвтоматическое обновление дополнений гостевой ОС в ВМ:\r\n```\r\n[root@vz ~]# prlctl set vm1 --tools-autoupdate on\r\n```\r\n\r\n### <a name='pause-vm'></a>Приостановка виртуальных машин\r\nКоманды управления контейнерами с помощью `prlctl` аналогично используются и для ВМ:\r\n* `set`\r\n* `list`\r\n* `start`\r\n* `stop`\r\n* `restart`\r\n* `suspend`\r\n* `resume`\r\n* `exec`\r\n* `enter`\r\n* `console`\r\n* `status`\r\n* `create`\r\n* `delete`\r\n* `mount`\r\n* `umount`\r\n* `clone`\r\n* `move`\r\n* `snapshot`\r\n\r\nВдобавок к этим командам существует возможность приостанавливать ВМ:\r\n```\r\n[root@vz ~]# prlctl pause vm1\r\nPause the VM...\r\nThe VM has been successfully paused.\r\n[root@vz ~]# prlctl status vm1\r\nVM vm1 exist paused\r\n[root@vz ~]# prlctl start vm1\r\nStarting the VM...\r\nThe VM has been successfully started.\r\n```\r\n\r\n### <a name='templates-vm'></a>Шаблоны конфигураций\r\nНа основе уже имеющихся виртуальных машин можно создавать типовые конфигурации.\r\n\r\nПример создания шаблона `config-1024MB-centos7`, основанный на ранее настроенной `vm1`:\r\n```\r\n[root@vz ~]# mkdir /etc/parallels/samples\r\n[root@vz ~]# cp /vz/vmprivate/vm1.pvm/config.pvs /etc/parallels/samples/config-1024MB-centos7.pvs\r\n[root@vz ~]# prlctl create vm3\r\nCreating the virtual machine...\r\nGenerate the VM configuration for win-2008.\r\nThe VM has been successfully created.\r\n[root@vz ~]# prlctl list vm3 -i | egrep \"cpu|memory|video|hdd0\"\r\nBoot order: hdd0 cdrom0\r\n  cpu cpus=1 VT-x accl=high mode=32 ioprio=4 iolimit='0'\r\n  memory 512Mb\r\n  video 32Mb 3d acceleration=highest vertical sync=yes\r\n  memory_guarantee auto\r\n  hdd0 (+) scsi:0 image='/vz/vmprivate/vm3.pvm/harddisk.hdd' type='expanded' 65536Mb subtype=virtio-scsi\r\n[root@vz ~]# prlctl set vm3 --applyconfig config-1024MB-centos7\r\n[root@vz ~]# prlctl list vm3 -i | egrep \"cpu|memory|video|hdd0\"\r\nBoot order: hdd0 cdrom0\r\n  cpu cpus=2 VT-x accl=high mode=32 cpuunits=1000 cpulimit=1024Mhz ioprio=4 iolimit='0' mask=0-1\r\n  memory 1024Mb\r\n  video 64Mb 3d acceleration=highest vertical sync=yes\r\n  memory_guarantee auto\r\n  hdd0 (+) scsi:0 image='/vz/vmprivate/vm3.pvm/harddisk.hdd' type='expanded' 8192Mb subtype=virtio-scsi\r\n```\r\n\r\n### <a name='devices-vm'></a>Добавление и удаление устройств в ВМ\r\nДля каждой виртуальной машины доступно максимальное количество устройств:\r\n* 4 IDE или 8 SCSI (HDD, CD/DVD-ROM) устройств\r\n* 16 сетевых адаптеров\r\n* 4 последовательных (COM) порта\r\n* один USB-контроллер\r\n* одно FDD устройство\r\n\r\nДобавление дополнительного HDD для виртуальной машины с Linux:\r\n```\r\n[root@vz ~]# prlctl list vm1 -i | grep hdd\r\nBoot order: hdd0 cdrom1\r\n  hdd0 (+) scsi:0 image='/vz/vmprivate/vm1.pvm/harddisk.hdd' type='expanded' 8192Mb subtype=virtio-scsi\r\n[root@vz ~]# prlctl set vm1 --device-add hdd --size 2048 --iface scsi\r\nCreating hdd1 (+) scsi:2 image='/vz/vmprivate/vm1.pvm/harddisk1.hdd' type='expanded' 2048Mb subtype=virtio-scsi\r\nCreated hdd1 (+) scsi:2 image='/vz/vmprivate/vm1.pvm/harddisk1.hdd' type='expanded' 2048Mb subtype=virtio-scsi\r\n[root@vz ~]# prlctl list vm1 -i | grep hdd\r\nBoot order: hdd0 cdrom1 hdd1\r\n  hdd0 (+) scsi:0 image='/vz/vmprivate/vm1.pvm/harddisk.hdd' type='expanded' 8192Mb subtype=virtio-scsi\r\n  hdd1 (+) scsi:2 image='/vz/vmprivate/vm1.pvm/harddisk1.hdd' type='expanded' 2048Mb subtype=virtio-scsi\r\n```\r\n\r\nПосле того как диск добавлен, нужно создать на нем раздел и отформатировать его:\r\n```\r\n[root@vz ~]# prlctl enter vm1\r\n[root@vm1 /]# lsblk | grep sd\r\nsda               8:0    0    8G  0 disk\r\n├─sda1            8:1    0  500M  0 part /boot\r\n└─sda2            8:2    0  7.5G  0 part\r\nsdb               8:16   0    2G  0 disk\r\n[root@vm1 /]# fdisk /dev/sdb\r\nCommand (m for help): n\r\nPartition type:\r\n   p   primary (0 primary, 0 extended, 4 free)\r\n   e   extended\r\nSelect (default p): p\r\nPartition number (1-4, default 1): <Enter>\r\nFirst sector (2048-4194303, default 2048): <Enter>\r\nLast sector, +sectors or +size{K,M,G} (2048-4194303, default 4194303): <Enter>\r\nCommand (m for help): w\r\n[root@vm1 /]# mkfs -t ext4 /dev/sdb1\r\n[root@vm1 /]# mount /dev/sdb1 /mnt/\r\n[root@vm1 /]# tail -1 /etc/mtab >> /etc/fstab\r\n```\r\n\r\nС помощью ключа `--device-disconnect` можно отключить устройство от ВМ:\r\n```\r\n[root@vz ~]# prlctl set vm1 --device-disconnect cdrom1\r\nDisconnect device: cdrom1\r\nThe device successfully disconnected\r\n[root@vz ~]# prlctl list vm1 -i | grep cdrom1\r\nBoot order: hdd0 cdrom1 hdd1\r\n  cdrom1 (+) scsi:1 image='/vz/vmprivate/images/CentOS-7-x86_64-Minimal-1503-01.iso' state=disconnected subtype=virtio-scsi\r\n```\r\n\r\nВключить устройство можно воспользовавшись ключом `--device-connect`.\r\n\r\nЧтобы полностью удалить устройство, нужно использовать ключ `--device-del`:\r\n```\r\n[root@vz ~]# prlctl set vm1 --device-del usb\r\nRemove the usb device.\r\n[root@vz ~]# prlctl set vm1 --device-del cdrom1\r\nRemove the cdrom1 device.\r\n[root@vz ~]# prlctl list vm1 -i | egrep \"cdrom1|usb\"\r\n```\r\n\r\nПри удалении HDD из виртуальной машины можно сохранить сам виртуальный диск, для этого используется ключ `--detach-only`, по умолчанию диск удаляется, что является умолчанием ключа `--destroy-image`.\r\n\r\nИзменение приоритета загрузки устройств:\r\n```\r\n[root@vz ~]# prlctl list vm1 -i | grep \"Boot order\"\r\nBoot order: hdd0 hdd1\r\n[root@vz ~]# prlctl set vm1 --device-bootorder \"hdd1 hdd0\"\r\n[root@vz ~]# prlctl list vm1 -i | grep \"Boot order\"\r\nBoot order: hdd1 hdd0\r\n```\r\n\r\nПример добавления дополнительного сетевого устройства:\r\n```\r\n[root@vz ~]# prlctl set vm1 --device-add net --network Bridged  --mac auto --ipadd 192.168.0.181 --gw 192.168.122.1 --nameserver 192.168.0.1 --adapter-type virtio\r\nEnable automatic reconfiguration for this network adapter.\r\nCreating net1 (+) dev='' ifname='eth1' network='Bridged' mac=001C42AFDC9B card=virtio ips='192.168.0.181/255.255.255.0 ' gw='192.168.0.1'\r\nCreated net1 (+) dev='vme42afdc9b' network='Bridged' mac=001C42AFDC9B card=virtio ips='192.168.0.181/255.255.255.0 ' gw='192.168.0.1'\r\n[root@vz ~]# prlctl list vm1 -i | grep -i net\r\n  net0 (+) dev='vme4292dc5f' network='Bridged' mac=001C4292DC5F card=virtio ips='192.168.0.180/255.255.255.0 FE80:0:0:0:20C:29FF:FE01:FB07/64 '\r\n  net1 (+) dev='vme42afdc9b' network='Bridged' mac=001C42AFDC9B card=virtio ips='192.168.0.181/255.255.255.0 ' gw='192.168.0.1'\r\n```\r\n\r\n### <a name='hotplug-vm'></a>Горячее подключение CPU и RAM\r\nДля виртуальных машин доступно горячее подключение (hotplug) ресурсов без перезагрузки самих виртуальных машин.\r\nК таким ресурсам относятся оперативная память и процессор.\r\nПо умолчанию для виртуальных машин горячее подключение отключено.\r\n\r\nДля добавления оперативной памяти \"налету\" необходимо установить значение ключа `--mem-hotplug` в `on`:\r\n```\r\n[root@vz ~]# prlctl list vm1 -i | grep \"memory \"\r\n  memory 1024Mb\r\n[root@vz ~]# prlctl set vm1 --mem-hotplug on\r\nset mem hotplug: 1\r\n```\r\n\r\nПосле установки параметра нужно единожды перезагрузить виртуальную машину и затем изменять количество памяти:\r\n```\r\n[root@vz ~]# prlctl restart vm1\r\nRestarting the VM...\r\nThe VM has been successfully restarted.\r\n[root@vz ~]# prlctl set vm1 --memsize 1536M\r\nSet the memsize parameter to 1536Mb.\r\n[root@vz ~]# prlctl list vm1 -i | grep \"memory \"\r\n  memory 1536Mb hotplug\r\n```\r\n\r\nДля включения CPU hotplug, необходимо чтобы операционная система гостевой ВМ поддерживала данную функцию.\r\nНа данный момент поддерживаются:\r\n* дистрибутивы основанные на RHEL 5 и выше\r\n* Windows Server 2008 x64 и выше\r\n\r\nВключение CPU hotplug происходит по аналогии с MEM hotplug:\r\n```\r\n[root@vz ~]# prlctl list vm1 -i | grep cpu\r\n  cpu cpus=2 VT-x accl=high mode=32 cpuunits=1000 cpulimit=1024Mhz ioprio=6 iolimit='0' mask=0-1\r\n[root@vz ~]# prlctl set vm1 --cpu-hotplug on\r\n  set cpu hotplug: 1\r\n```\r\n\r\nПерезагрузка виртуальной машины и проверка ресурсов:\r\n```\r\n[root@vz ~]# prlctl restart vm1\r\nRestarting the VM...\r\nThe VM has been successfully restarted.\r\n[root@vz ~]# prlctl set vm1 --cpuunits 2000 --cpus 3 --cpumask 0-2\r\nset cpus(4): 3\r\nset cpuunits 2000\r\nset cpu mask 0-2\r\n[root@vz ~]# prlctl list vm1 -i | grep cpu\r\n  cpu cpus=3 VT-x hotplug accl=high mode=32 cpuunits=2000 cpulimit=1024Mhz ioprio=6 iolimit='0' mask=0-2\r\n```\r\n\r\nДля отключения hotplug используется значения ключей `--mem-hotplug off` и `--cpu-hotplug off` соответственно.\r\n\r\n### <a name='ksm'></a>Оптимизация виртуальных машин с помощью KSM\r\nKSM (Kernel Same-Page Merging) — технология ядра Linux, которая позволяет ядру объединять одинаковые страницы памяти между различными процессами или виртуальными гостевыми системами в одну для совместного использования.\r\n\r\nТехнология предусматривает сканирование памяти в целях поиска дубликатов страниц, каждая пара дубликатов объединяется в одну страницу, помечаемую как подлежащую копированию при записи, таким образом ядро будет автоматически разделять страницы памяти, как только один процесс изменит данные.\r\n\r\nПоиск дубликатов накладывает дополнительную нагрузку на процессорные ресурсы, поэтому в технологии была предусмотрена оценка — насколько часто будут те или иные страницы памяти изменяться, чтобы в тех случаях, когда ресурсные издержки сравнительно высоки не прибегать к слиянию страниц.\r\nKSM обеспечивает системам виртуализации возможность перезакладки ресурсов оперативной памяти (memory overcommitment).\r\nВ условиях достаточно однородных экземпляров гостевых операционных систем возможен значительный эффект, в частности, экспериментальная реализация KSM от Red Hat показала, что 52 виртуальных экземпляра Windows XP с выделенными 1GB памяти, могут работать на хост-ноде с 16GB оперативной памяти.\r\n\r\nДля включения KSM в OpenVZ необходимо запустить сервисы `ksm` и `ksmtuned`:\r\n```\r\n[root@vz ~]# systemctl start ksm && systemctl enable ksm\r\n[root@vz ~]# systemctl start ksmtuned && systemctl enable ksmtuned\r\n```\r\n\r\nРаботоспособность KSM можно проверить на примере общих страниц в памяти (shared memory pages):\r\n```\r\n[root@vz ~]# cat /sys/kernel/mm/ksm/pages_sharing\r\n120990\r\n```\r\n\r\n## [[⬆]](#toc) <a name='migration'></a>Миграция контейнеров и виртуальных машин\r\nВ OpenVZ поддерживается \"живая\" миграция контейнеров и виртуальных машин с использованием CRIU и P.Haul.\r\n\r\nПример миграции контейнера `ct3` с хост-ноды `vz-source` на `vz-dest` (192.168.0.180).\r\n\r\nСоздаем и копируем SSH-ключ с `vz-source` на `vz-dest` для беспарольной аутентификации:\r\n```\r\n[root@vz-source ~]# ssh-keygen\r\n[root@vz-source ~]# ssh-copy-id root@192.168.0.180\r\n```\r\n\r\nЗапускаем миграцию в `screen`:\r\n```\r\n[root@vz-source ~]# screen -S migrate-dest\r\n[root@vz-source ~]# prlctl migrate ct3 192.168.0.180\r\n```\r\n\r\nПроверка на `vz-dest` смигрированного контейнера:\r\n```\r\n[root@vz-dest ~]# prlctl list ct3\r\nUUID                                    STATUS       IP_ADDR         T  NAME\r\n{4730cba8-deed-4168-9f9e-34373e618026}  running      192.168.0.163   CT ct3\r\n```\r\n\r\n## [[⬆]](#toc) <a name='extra-info'></a>Расширенная информация о контейнерах и ВМ\r\nПодробную информацию о контейнере или виртуальной машине можно получить с помощью параметра `list` с ключом `-i` (`--info`):\r\n```\r\n[root@vz ~]# prlctl list -i ct4\r\nINFO\r\nID: {22c418d7-948b-456e-9d84-d59ab5ead661}\r\nEnvID: 22c418d7-948b-456e-9d84-d59ab5ead661\r\nName: ct4\r\nDescription:\r\nType: CT\r\nState: running\r\nOS: centos7\r\nTemplate: no\r\nUptime: 00:00:00 (since 2016-02-09 17:04:41)\r\nHome: /vz/private/22c418d7-948b-456e-9d84-d59ab5ead661\r\nOwner: root\r\nEffective owner: owner\r\nGuestTools: state=possibly_installed\r\nAutostart: on\r\nAutostop: suspend\r\nAutocompact: on\r\nUndo disks: off\r\nBoot order:\r\nEFI boot: off\r\nAllow select boot device: off\r\nExternal boot device:\r\nRemote display: mode=off address=0.0.0.0\r\nRemote display state: stopped\r\nHardware:\r\n  cpu cpus=unlimited VT-x accl=high mode=32 cpuunits=1000 ioprio=4\r\n  memory 512Mb\r\n  video 0Mb 3d acceleration=highest vertical sync=yes\r\n  memory_guarantee auto\r\n  hdd0 (+) image='/vz/private/22c418d7-948b-456e-9d84-d59ab5ead661/root.hdd' type='expanded' 10240Mb mnt=/\r\n  venet0 (+) type='routed' ips='192.168.0.164/255.255.255.0 FE80:0:0:0:20C:29FF:FE01:FB10/64 '\r\nHost Shared Folders: (-)\r\nFeatures:\r\nEncrypted: no\r\nFaster virtual machine: on\r\nAdaptive hypervisor: off\r\nDisabled Windows logo: on\r\nAuto compress virtual disks: on\r\nNested virtualization: off\r\nPMU virtualization: off\r\nOffline management: (-)\r\nHostname: ct4.vz.localhost\r\nDNS Servers: 192.168.0.1 192.168.0.2\r\nSearch Domains: 192.168.0.1\r\n```\r\n\r\nСуществует также возможность просмотра дополнительной информации о контейнерах:\r\n```\r\n[root@vz ~]# prlctl list -o type,status,name,hostname,dist,ip\r\nT  STATUS       NAME     HOSTNAME             DIST         IP_ADDR\r\nCT running      ct2      ct2.vz.localhost     debian       192.168.0.162 FE80:0:0:0:20C:29FF:FE01:FB09\r\nCT running      ct1      ct1.vz.localhost     debian       192.168.0.161 FE80:0:0:0:20C:29FF:FE01:FB08\r\nVM stopped      vm1      vm1.vz.localhost     rhel7        192.168.0.163 FE80:0:0:0:20C:29FF:FE01:FB07\r\n```\r\n\r\nСписок всех доступных полей:\r\n```\r\n[root@vz ~]# prlctl list -L\r\nuuid                 UUID\r\nenvid                ENVID\r\nstatus               STATUS\r\nname                 NAME\r\ndist                 DIST\r\nowner                OWNER\r\nsystem-flags         SYSTEM_FLAGS\r\ndescription          DESCRIPTION\r\nnumproc              NPROC\r\nip                   IP_ADDR\r\nip_configured        IP_ADDR\r\nhostname             HOSTNAME\r\nnetif                NETIF\r\nmac                  MAC\r\nfeatures             FEATURES\r\nlocation             LOCATION\r\niolimit              IOLIMIT\r\nnetdev               NETDEV\r\ntype                 T\r\nha_enable            HA_ENABLE\r\nha_prio              HA_PRIO\r\n-                    -\r\n```\r\n\r\n## [[⬆]](#toc) <a name='recommendations'></a>Рекомендации системному администратору\r\n* если работа хост-ноды замедлилась, для анализа нагрузки можно воспользоваться утилитами `vzps`, `vztop`, `iotop`, `atop`\r\n* для обнаружения сетевых проблем можно воспользоваться утилитами `ping`, `traceroute`, `nmap`, `mtr`, `tcpdump`, `nc`, `iftop`, `netstat`, `ss`\r\n* `strace` будет верным помощником для отслеживания системных вызовов\r\n* используйте RAID для обеспечения сохранности данных\r\n* для экстренных ситуаций, когда могут быть проблемы с сетью необходимо всегда иметь под рукой IPMI или KVM-доступ\r\n* не перезагружайте хост-ноду без выяснения обстоятельств неполадок, делайте это только в самых крайних случаях\r\n* следите за временем на сервере, используйте для синхронизации NTP на хост-ноде\r\n* в контейнерах нет смысла устанавливать второй экземпляр NTP, достаточно только указать нужный часовой пояс\r\n* не запускайте блобы или скрипты, которые принадлежат VPS, непосредственно с хост-ноды\r\n* для поиска руткитов можно воспользоваться утилитами `chrootkit` и `rkhunter`\r\n* следите за нагрузкой сервера, обезопасьтесь от DoS/DDoS\r\n* используйте системы конфигураций, такие как Puppet, Ansible, Chef, SaltStack, CFEngine если используется большое количество однотипных конфигураций на хост-нодах\r\n* делайте резервные копии важных данных\r\n* следите за свободным местом на жестких дисках, используйте ротацию логов\r\n* следите за пулом IP-адресов, они могут закончиться в самый неподходящий момент\r\n* проверяйте каталог `/var/log/`, который содержит логи системы\r\n* используйте `iptables`, `fail2ban`, `ipset`\r\n* генерируйте сложные для перебора пароли (например с помощью `pwgen`), периодически меняйте их, уведомляйте пользователей об этом\r\n* внимательно работайте на сервере под учетной записью `root`\r\n* следите за рассылками новостей по безопасности\r\n* обновляйте ПО, систему и ее компоненты\r\n* следите за правами пользователей, файлов и каталогов на сервере\r\n* используйте системы мониторинга ресурсов (например Cacti, Munin, MRTG, Zabbix, Nagios, Icinga, Monit)\r\n* ведите внутреннюю документацию по серверам и их настройке\r\n* в случае обнаружения проблем, можно обратиться к документации проектов OpenVZ и Virtuozzo, а также задать вопросы на тематических форумах\r\n\r\n## [[⬆]](#toc) <a name='links'></a>Ссылки\r\n* https://docs.openvz.org\r\n* https://src.openvz.org/projects\r\n* https://wiki.openvz.org/Main_Page\r\n* https://lists.openvz.org/mailman/listinfo/devel\r\n* https://bugs.openvz.org/secure/Dashboard.jspa\r\n\r\n## [[⬆]](#toc) <a name='todo'></a>TODO\r\n* управление сетью в OpenVZ (veth/vlan/shaping)\r\n* проброс устройств (usb/vlan) (https://habrahabr.ru/post/210460/)\r\n* `prlctl` для управления дисковыми квотами, `--diskinodes` для `prlctl` не работает (https://bugs.openvz.org/browse/OVZ-6717) и (https://bugs.openvz.org/browse/OVZ-6505)\r\n* некоторые ключи для `prlctl set`: `--3d-accelerate` `--vertical-sync` `--memguarantee` `--autostop` `--start-as-user` `--autostart-delay` `--autostop` `--autocompact`\r\n* создание шаблона гостевой ОС на основе vztt/vzmktmpl\r\n* не работает `prlctl capture` (возможно нужно протестировать с иксами)\r\n* `vztop`/`vzps`/`vzstat`/`vznetstat` работают только с UUID (https://bugs.openvz.org/browse/OVZ-6504)\r\n* не создается снапшот со включенным nfsd (https://bugs.openvz.org/browse/OVZ-6780)\r\n\r\n## [[⬆]](#toc) <a name='license'></a>Лицензия\r\n![CC BY-SA 4.0](https://licensebuttons.net/l/by-sa/4.0/88x31.png)\r\n\r\n[Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/deed.ru)\r\n",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}